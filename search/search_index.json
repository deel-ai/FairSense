{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Explore FairSense docs This library allow to compute global sensitivity indices in the context of fairness measurements. The paper Fairness seen as Global Sensitivity Analysis bridges the gap between global sensitivity analysis (GSA) and fairness. It states that for each sensitivity analysis, there is a fairness measure, and vice-versa. @misc{https://doi.org/10.48550/arxiv.2103.04613, doi = {10.48550/ARXIV.2103.04613}, url = {https://arxiv.org/abs/2103.04613}, author = {B\u00e9nesse, Cl\u00e9ment and Gamboa, Fabrice and Loubes, Jean-Michel and Boissin, Thibaut}, keywords = {Statistics Theory (math.ST), Methodology (stat.ME), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences}, title = {Fairness seen as Global Sensitivity Analysis}, This library is a toolbox which ease the computation of fairness and GSA indices. \ud83d\udc49 The problem \u00b6 Each index has it's characteristics: some can be applied on continuous variables and some cannot. Some can handle regression problems and some handle classification problems. Some can handle variable groups and some cannot. Finally some can only be applied on the predictions of a model while some can be applied on the error made by the model. The objective is then to provide a tool to investigate the fairness of an ML problem by computing the GSA indices while avoiding the aforementioned issues. \ud83d\ude80 The strategy \u00b6 The library allows to formulate a fairness problem which is stated as following: a dataset describing the training distribution a model which can be a function or a machine learning model a fairness objective which indicate what should be studied : one can study the intrinsic bias of a dataset, or the bias of the model or the bias of the model's errors These elements are encapsulated in an object called IndicesInput . Then it becomes possible to compute GSA indices (in a interchangeable way) using the functions provided in fairsense.indices . These functions output IndicesOutput objects that encapsulate the values of the indices. These results can finally be visualized with the functions available in the fairsense.visualization module. \ud83d\udcbb install fairsense \u00b6 \u200dfor users \u00b6 pip install fairsense for developpers \u00b6 After cloning the repository pip install -e . [ dev ] to clean code, at the root of the lib: black . for docs \u00b6 pip install -e . [ docs ] build rst files, in the docs folder: sphinx-apidoc .. \\l ibfairness -o source the generate html docs: make html Warning: the library must be installed to generate the doc. \ud83d\udc4d Contributing \u00b6 Feel free to propose your ideas or come and contribute with us on the Libname toolbox! We have a specific document where we describe in a simple way how to make your first pull request: just here . \ud83d\udc40 See Also \u00b6 More from the DEEL project: Xplique a Python library exclusively dedicated to explaining neural networks. deel-lip a Python library for training k-Lipschitz neural networks on TF. Influenciae Python toolkit dedicated to computing influence values for the discovery of potentially problematic samples in a dataset. deel-torchlip a Python library for training k-Lipschitz neural networks on PyTorch. DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of data quality, representativity and explainability for this purpose. \ud83d\ude4f Acknowledgments \u00b6 This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the DEEL project. \ud83d\uddde\ufe0f Citation \u00b6 If you use fairsense as part of your workflow in a scientific publication, please consider citing the \ud83d\uddde\ufe0f our paper : @misc{https://doi.org/10.48550/arxiv.2103.04613, doi = {10.48550/ARXIV.2103.04613}, url = {https://arxiv.org/abs/2103.04613}, author = {B\u00e9nesse, Cl\u00e9ment and Gamboa, Fabrice and Loubes, Jean-Michel and Boissin, Thibaut}, keywords = {Statistics Theory (math.ST), Methodology (stat.ME), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences}, title = {Fairness seen as Global Sensitivity Analysis}, \ud83d\udcdd License \u00b6 The package is released under MIT license . \ud83d\udca3 Disclaimer \u00b6 To the maximum extent permitted by applicable law, authors of FairSense shall not be liable for any kind of tangible and intangible damages. Especially the authors shall not be liable in case of incorrect computation of the indices nor any biased interpretation of such indices.","title":"Home"},{"location":"#the-problem","text":"Each index has it's characteristics: some can be applied on continuous variables and some cannot. Some can handle regression problems and some handle classification problems. Some can handle variable groups and some cannot. Finally some can only be applied on the predictions of a model while some can be applied on the error made by the model. The objective is then to provide a tool to investigate the fairness of an ML problem by computing the GSA indices while avoiding the aforementioned issues.","title":"\ud83d\udc49 The problem"},{"location":"#the-strategy","text":"The library allows to formulate a fairness problem which is stated as following: a dataset describing the training distribution a model which can be a function or a machine learning model a fairness objective which indicate what should be studied : one can study the intrinsic bias of a dataset, or the bias of the model or the bias of the model's errors These elements are encapsulated in an object called IndicesInput . Then it becomes possible to compute GSA indices (in a interchangeable way) using the functions provided in fairsense.indices . These functions output IndicesOutput objects that encapsulate the values of the indices. These results can finally be visualized with the functions available in the fairsense.visualization module.","title":"\ud83d\ude80 The strategy"},{"location":"#install-fairsense","text":"","title":"\ud83d\udcbb install fairsense"},{"location":"#for-users","text":"pip install fairsense","title":"\u200dfor users"},{"location":"#for-developpers","text":"After cloning the repository pip install -e . [ dev ] to clean code, at the root of the lib: black .","title":"for developpers"},{"location":"#for-docs","text":"pip install -e . [ docs ] build rst files, in the docs folder: sphinx-apidoc .. \\l ibfairness -o source the generate html docs: make html Warning: the library must be installed to generate the doc.","title":"for docs"},{"location":"#contributing","text":"Feel free to propose your ideas or come and contribute with us on the Libname toolbox! We have a specific document where we describe in a simple way how to make your first pull request: just here .","title":"\ud83d\udc4d Contributing"},{"location":"#see-also","text":"More from the DEEL project: Xplique a Python library exclusively dedicated to explaining neural networks. deel-lip a Python library for training k-Lipschitz neural networks on TF. Influenciae Python toolkit dedicated to computing influence values for the discovery of potentially problematic samples in a dataset. deel-torchlip a Python library for training k-Lipschitz neural networks on PyTorch. DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of data quality, representativity and explainability for this purpose.","title":"\ud83d\udc40 See Also"},{"location":"#acknowledgments","text":"This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the DEEL project.","title":"\ud83d\ude4f Acknowledgments"},{"location":"#citation","text":"If you use fairsense as part of your workflow in a scientific publication, please consider citing the \ud83d\uddde\ufe0f our paper : @misc{https://doi.org/10.48550/arxiv.2103.04613, doi = {10.48550/ARXIV.2103.04613}, url = {https://arxiv.org/abs/2103.04613}, author = {B\u00e9nesse, Cl\u00e9ment and Gamboa, Fabrice and Loubes, Jean-Michel and Boissin, Thibaut}, keywords = {Statistics Theory (math.ST), Methodology (stat.ME), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences}, title = {Fairness seen as Global Sensitivity Analysis},","title":"\ud83d\uddde\ufe0f Citation"},{"location":"#license","text":"The package is released under MIT license .","title":"\ud83d\udcdd License"},{"location":"#disclaimer","text":"To the maximum extent permitted by applicable law, authors of FairSense shall not be liable for any kind of tangible and intangible damages. Especially the authors shall not be liable in case of incorrect computation of the indices nor any biased interpretation of such indices.","title":"\ud83d\udca3 Disclaimer"},{"location":"CONTRIBUTING/","text":"Contributing \u00b6 Thanks for taking the time to contribute! From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first . This way we can ensure that your precious work is not in vain. Setup with make \u00b6 Clone the repo git clone https://github.com/deel-ai/FairSense.git . Go to your freshly downloaded repo cd FairSense Create a virtual environment and install the necessary dependencies for development: make prepare-dev && source fairsense/bin/activate . Welcome to the team ! Tests \u00b6 To run test make test This command activate your virtual environment and launch the tox command. tox on the otherhand will do the following: - run pytest on the tests folder with python 3.6, python 3.7 and python 3.8 Note: If you do not have those 3 interpreters the tests would be only performs with your current interpreter - run pylint on the deel-datasets main files, also with python 3.6, python 3.7 and python 3.8 Note: It is possible that pylint throw false-positive errors. If the linting test failed please check first pylint output to point out the reasons. Please, make sure you run all the tests at least once before opening a pull request. A word toward Pylint for those that don't know it: Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. Basically, it will check that your code follow a certain number of convention. Any Pull Request will go through a Github workflow ensuring that your code respect the Pylint conventions (most of them at least). Submitting Changes \u00b6 After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly (see Governance policy ). Something that will increase the chance that your pull request is accepted: Write tests and ensure that the existing ones pass. If make test is succesful, you have fair chances to pass the CI workflows (linting and test) Follow the existing coding style and run make check_all to check all files format. Write a good commit message (we follow a lowercase convention). For a major fix/feature make sure your PR has an issue and if it doesn't, please create one. This would help discussion with the community, and polishing ideas in case of a new feature.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"Thanks for taking the time to contribute! From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first . This way we can ensure that your precious work is not in vain.","title":"Contributing"},{"location":"CONTRIBUTING/#setup-with-make","text":"Clone the repo git clone https://github.com/deel-ai/FairSense.git . Go to your freshly downloaded repo cd FairSense Create a virtual environment and install the necessary dependencies for development: make prepare-dev && source fairsense/bin/activate . Welcome to the team !","title":"Setup with make"},{"location":"CONTRIBUTING/#tests","text":"To run test make test This command activate your virtual environment and launch the tox command. tox on the otherhand will do the following: - run pytest on the tests folder with python 3.6, python 3.7 and python 3.8 Note: If you do not have those 3 interpreters the tests would be only performs with your current interpreter - run pylint on the deel-datasets main files, also with python 3.6, python 3.7 and python 3.8 Note: It is possible that pylint throw false-positive errors. If the linting test failed please check first pylint output to point out the reasons. Please, make sure you run all the tests at least once before opening a pull request. A word toward Pylint for those that don't know it: Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. Basically, it will check that your code follow a certain number of convention. Any Pull Request will go through a Github workflow ensuring that your code respect the Pylint conventions (most of them at least).","title":"Tests"},{"location":"CONTRIBUTING/#submitting-changes","text":"After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly (see Governance policy ). Something that will increase the chance that your pull request is accepted: Write tests and ensure that the existing ones pass. If make test is succesful, you have fair chances to pass the CI workflows (linting and test) Follow the existing coding style and run make check_all to check all files format. Write a good commit message (we follow a lowercase convention). For a major fix/feature make sure your PR has an issue and if it doesn't, please create one. This would help discussion with the community, and polishing ideas in case of a new feature.","title":"Submitting Changes"},{"location":"api/data_management_factory/","text":"This module contains factory functions that allow to build more easily IndicesInput objects. from_numpy ( x , y , feature_names = None , model = None , target = None ) \u00b6 Builds IndicesInput from numpy array. Parameters: Name Type Description Default x numpy array containing the samples to analyse. required y numpy array containing the labels. Can be None if no labels are provided. required feature_names a list of str containing the features name of x. When None features are named with numbers. None model function that can be applied on dataframe, that return an series with same shape as y. None target one of the target from the utils.fairness_objective module. None Returns: Type Description IndicesInput an IndicesInput object that can be used to compute sensitivity indices. Source code in deel\\fairsense\\data_management\\factory.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def from_numpy ( x , y , feature_names = None , model = None , target = None ) -> IndicesInput : \"\"\" Builds IndicesInput from numpy array. Args: x: numpy array containing the samples to analyse. y: numpy array containing the labels. Can be None if no labels are provided. feature_names: a list of str containing the features name of x. When None features are named with numbers. model: function that can be applied on dataframe, that return an series with same shape as y. target: one of the target from the utils.fairness_objective module. Returns: an IndicesInput object that can be used to compute sensitivity indices. \"\"\" df = pd . DataFrame ( x , columns = feature_names ) # build dataframe y = pd . DataFrame ( y , columns = [ \"target\" ]) return from_pandas ( dataframe = df , y = y , model = model , target = target ) from_pandas ( dataframe , y , model = None , target = None ) \u00b6 Builds IndicesInput from pandas dataframe. Parameters: Name Type Description Default dataframe pd . DataFrame DataFrame containing the samples to analyse. required y Union [ str , pd . DataFrame , pd . Series , None] Union[str, pd.DataFrame, pd.Series, None] : when str, refers to the name of the columns containing the labels. Must be present in dataframe. When pd.DataFrame or pd.Series the label are provided in the same order as in dataframe. When None, no labels are provided. required model Optional [ Callable ] function that can be applied on dataframe, that return a series with same shape as y. None target Callable one of the target from the utils.fairness_objective module. None Returns: Type Description IndicesInput an IndicesInput object that can be used to compute sensitivity indices. Source code in deel\\fairsense\\data_management\\factory.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def from_pandas ( dataframe : pd . DataFrame , y : Union [ str , pd . DataFrame , pd . Series , None ], model : Optional [ Callable ] = None , target : Callable = None , ) -> IndicesInput : \"\"\" Builds IndicesInput from pandas dataframe. Args: dataframe: DataFrame containing the samples to analyse. y: Union[str, pd.DataFrame, pd.Series, None] : when str, refers to the name of the columns containing the labels. Must be present in dataframe. When pd.DataFrame or pd.Series the label are provided in the same order as in dataframe. When None, no labels are provided. model: function that can be applied on dataframe, that return a series with same shape as y. target: one of the target from the utils.fairness_objective module. Returns: an IndicesInput object that can be used to compute sensitivity indices. \"\"\" cols = set ( dataframe . columns ) if y is None : assert model is not None , \"model must be defined when target is None\" x = dataframe y = None elif isinstance ( y , str ): x = dataframe [ cols - { y }] y = dataframe [ y ] elif isinstance ( y , pd . DataFrame ) or isinstance ( y , pd . Series ): x = dataframe y = pd . DataFrame ( y ) else : raise RuntimeError ( \"type of target must be Dataframe, Series, str or None\" ) return IndicesInput ( x = x , y_true = y , model = model , objective = target )","title":"factory module"},{"location":"api/data_management_factory/#deel.fairsense.data_management.factory.from_numpy","text":"Builds IndicesInput from numpy array. Parameters: Name Type Description Default x numpy array containing the samples to analyse. required y numpy array containing the labels. Can be None if no labels are provided. required feature_names a list of str containing the features name of x. When None features are named with numbers. None model function that can be applied on dataframe, that return an series with same shape as y. None target one of the target from the utils.fairness_objective module. None Returns: Type Description IndicesInput an IndicesInput object that can be used to compute sensitivity indices. Source code in deel\\fairsense\\data_management\\factory.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def from_numpy ( x , y , feature_names = None , model = None , target = None ) -> IndicesInput : \"\"\" Builds IndicesInput from numpy array. Args: x: numpy array containing the samples to analyse. y: numpy array containing the labels. Can be None if no labels are provided. feature_names: a list of str containing the features name of x. When None features are named with numbers. model: function that can be applied on dataframe, that return an series with same shape as y. target: one of the target from the utils.fairness_objective module. Returns: an IndicesInput object that can be used to compute sensitivity indices. \"\"\" df = pd . DataFrame ( x , columns = feature_names ) # build dataframe y = pd . DataFrame ( y , columns = [ \"target\" ]) return from_pandas ( dataframe = df , y = y , model = model , target = target )","title":"from_numpy()"},{"location":"api/data_management_factory/#deel.fairsense.data_management.factory.from_pandas","text":"Builds IndicesInput from pandas dataframe. Parameters: Name Type Description Default dataframe pd . DataFrame DataFrame containing the samples to analyse. required y Union [ str , pd . DataFrame , pd . Series , None] Union[str, pd.DataFrame, pd.Series, None] : when str, refers to the name of the columns containing the labels. Must be present in dataframe. When pd.DataFrame or pd.Series the label are provided in the same order as in dataframe. When None, no labels are provided. required model Optional [ Callable ] function that can be applied on dataframe, that return a series with same shape as y. None target Callable one of the target from the utils.fairness_objective module. None Returns: Type Description IndicesInput an IndicesInput object that can be used to compute sensitivity indices. Source code in deel\\fairsense\\data_management\\factory.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def from_pandas ( dataframe : pd . DataFrame , y : Union [ str , pd . DataFrame , pd . Series , None ], model : Optional [ Callable ] = None , target : Callable = None , ) -> IndicesInput : \"\"\" Builds IndicesInput from pandas dataframe. Args: dataframe: DataFrame containing the samples to analyse. y: Union[str, pd.DataFrame, pd.Series, None] : when str, refers to the name of the columns containing the labels. Must be present in dataframe. When pd.DataFrame or pd.Series the label are provided in the same order as in dataframe. When None, no labels are provided. model: function that can be applied on dataframe, that return a series with same shape as y. target: one of the target from the utils.fairness_objective module. Returns: an IndicesInput object that can be used to compute sensitivity indices. \"\"\" cols = set ( dataframe . columns ) if y is None : assert model is not None , \"model must be defined when target is None\" x = dataframe y = None elif isinstance ( y , str ): x = dataframe [ cols - { y }] y = dataframe [ y ] elif isinstance ( y , pd . DataFrame ) or isinstance ( y , pd . Series ): x = dataframe y = pd . DataFrame ( y ) else : raise RuntimeError ( \"type of target must be Dataframe, Series, str or None\" ) return IndicesInput ( x = x , y_true = y , model = model , objective = target )","title":"from_pandas()"},{"location":"api/data_management_processing/","text":"one_hot_encode ( indices_input , categorical_variables = None ) \u00b6 Performs one-hot encoding on the specified categorical variables. Variable groups are updated accordingly. Newly created variables are named: original_feature_name=value Parameters: Name Type Description Default indices_input IndicesInput IndiceInput object containing the data. required categorical_variables List [ str ] name of the variable that should be encoded. None Returns: Type Description IndicesInput the updated IndicesInput. Source code in deel\\fairsense\\data_management\\processing.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def one_hot_encode ( indices_input : IndicesInput , categorical_variables : List [ str ] = None ) -> IndicesInput : \"\"\" Performs one-hot encoding on the specified categorical variables. Variable groups are updated accordingly. Newly created variables are named: `original_feature_name=value` Args: indices_input: IndiceInput object containing the data. categorical_variables: name of the variable that should be encoded. Returns: the updated IndicesInput. \"\"\" x = indices_input . x orig_var_groups = indices_input . variable_groups out_x = pd . get_dummies ( x , prefix = categorical_variables , prefix_sep = \"=\" , dummy_na = False , drop_first = True , columns = categorical_variables , ) out_var_groups = [] # read the original groups for group in orig_var_groups : # iterate through the variable names in the group new_group = [] for c in group : # in the group seek for the new variables names if c in out_x . columns : # variable has not been one hot encoded yet new_group += [ c ] else : # variable has been one hot encoded, find the new variable names and # add it to the group new_group += list ( filter ( lambda cname : cname . startswith ( c + \"=\" ), out_x . columns ) ) # group is finished add the new group to groups out_var_groups . append ( new_group ) return IndicesInput ( x = out_x , y_true = indices_input . y_true , model = indices_input . model , variable_groups = out_var_groups , objective = indices_input . objective , )","title":"processing module"},{"location":"api/data_management_processing/#deel.fairsense.data_management.processing.one_hot_encode","text":"Performs one-hot encoding on the specified categorical variables. Variable groups are updated accordingly. Newly created variables are named: original_feature_name=value Parameters: Name Type Description Default indices_input IndicesInput IndiceInput object containing the data. required categorical_variables List [ str ] name of the variable that should be encoded. None Returns: Type Description IndicesInput the updated IndicesInput. Source code in deel\\fairsense\\data_management\\processing.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def one_hot_encode ( indices_input : IndicesInput , categorical_variables : List [ str ] = None ) -> IndicesInput : \"\"\" Performs one-hot encoding on the specified categorical variables. Variable groups are updated accordingly. Newly created variables are named: `original_feature_name=value` Args: indices_input: IndiceInput object containing the data. categorical_variables: name of the variable that should be encoded. Returns: the updated IndicesInput. \"\"\" x = indices_input . x orig_var_groups = indices_input . variable_groups out_x = pd . get_dummies ( x , prefix = categorical_variables , prefix_sep = \"=\" , dummy_na = False , drop_first = True , columns = categorical_variables , ) out_var_groups = [] # read the original groups for group in orig_var_groups : # iterate through the variable names in the group new_group = [] for c in group : # in the group seek for the new variables names if c in out_x . columns : # variable has not been one hot encoded yet new_group += [ c ] else : # variable has been one hot encoded, find the new variable names and # add it to the group new_group += list ( filter ( lambda cname : cname . startswith ( c + \"=\" ), out_x . columns ) ) # group is finished add the new group to groups out_var_groups . append ( new_group ) return IndicesInput ( x = out_x , y_true = indices_input . y_true , model = indices_input . model , variable_groups = out_var_groups , objective = indices_input . objective , )","title":"one_hot_encode()"},{"location":"api/indices/","text":"Statistical Parity - Disparate Impact - Demographic Parity \u00b6 The rates of value-1 predictions from the different groups must be equal. Independence between the predictor and the protected variable. S binary \\(P(f(X)=1|S=0) = P(f(X)=1|S=1)\\) S continuous or discrete \\(P(f(X)=1|S) = P(f(X)=1)\\) Avoiding of Disparate Treatment \u00b6 The probability that an input leads to prediction 1 should be equal regardless of the value of the sensitive variable. S binary \\(P(f(X)=1|X_S=x,S=0) = P(f(X)=1|X_S=x,S=1)\\) where \\(X_S\\) represents \\(X\\) without the sensitive variable. Equality of Odds \u00b6 The rates of true and false predictions from the different groups must be equal. Independence between the error of the model and the protected variable. S binary \\(P(f(X)=1|Y=i,S=0) = P(f(X)=1|Y=i,S=1) ,i=0,1\\) S general \\(P(f(X)=1|Y=i,S) = P(f(X)=1|Y=i) ,i=0,1\\) Avoiding of Disparate Mistreatment \u00b6 The probability that a prediction is false should be equal regardless of the value of the sensitive variable. - S binary \\(P(f(X)\\ne Y|S=1) = P(f(X)\\ne Y|S=0)\\) Global Sensitivity Analysis \u00b6 GSA is used for quantifying the influence of a set of features on the outcome. Sobol' indices are based on correlations and need access to the function while CVM' indices are based on rank and need only a sample of evaluations. Sobol' indices 4 indices that quantify how much of the output variance can be explained by the variance of Xi. Correlation Between Variables Joined Contributions \\(Sob_i\\) \u2714\ufe0f \u274c \\(SobT_i\\) \u2714\ufe0f \u2714\ufe0f \\(Sob_i^{ind}\\) \u274c \u274c \\(SobT_i^{ind}\\) \u274c \u2714\ufe0f Cramer-Von Mises' indices The 2 CVM' indices is an extension of the Sobol\u2019 indices to quantify more than just the second-order influence of the inputs on the output. For further details about GSA in Fairness Case-of-use Recap \u00b6 Disparate Impact Avoiding Disparate Treatment Equality Odds Avoiding Disparate Mistreatment Sobol' indices Cramer-Von Mises' indices S binary \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f S discrete \u2714\ufe0f \u274c \u2714\ufe0f \u274c \u2714\ufe0f \u2714\ufe0f S continuous \u2714\ufe0f \u274c \u2714\ufe0f \u274c \u2714\ufe0f \u2714\ufe0f disparate_impact ( index_input , group_reduction = np . mean ) \u00b6 Compute the disparate impact. Warning disparate impact/equality of odds can only be computed on classification problems, and on categorical variables. Continuous variables are dropped and output replaced by np.nan Note When applied with target=classification_error this function compute the equality of odds. Parameters: Name Type Description Default index_input IndicesInput The fairness problem to study. required group_reduction the method used to compute the indices for a group of variables. By default the average of the values of each groups is applied. np.mean Returns: Type Description IndicesOutput IndicesOutput object, containing the CVM indices, one line per variable group IndicesOutput and one column for each index. Source code in deel\\fairsense\\indices\\standard_metrics.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def disparate_impact ( index_input : IndicesInput , group_reduction = np . mean ) -> IndicesOutput : \"\"\" Compute the disparate impact. Warning: disparate impact/equality of odds can only be computed on classification problems, and on categorical variables. Continuous variables are dropped and output replaced by `np.nan` Note: When applied with `target=classification_error` this function compute the equality of odds. Args: index_input (IndicesInput): The fairness problem to study. group_reduction: the method used to compute the indices for a group of variables. By default the average of the values of each groups is applied. Returns: IndicesOutput object, containing the CVM indices, one line per variable group and one column for each index. \"\"\" df = index_input . x y = index_input . compute_objective () df [ \"outputs\" ] = y . values if hasattr ( y , \"values\" ) else y dis = [] for group in index_input . variable_groups : group_output = [] for var in group : group_output . append ( _disparate_impact_single_variable ( df , var )) dis . append ( group_reduction ( group_output )) data = np . expand_dims ( np . array ( dis ), axis =- 1 ) index = index_input . merged_groups results = pd . DataFrame ( data = data , columns = [ \"DI\" ], index = index ) return IndicesOutput ( results ) sobol_indices ( inputs , n = 1000 , N = None ) \u00b6 Compute all sobol indices for all variables Warning this indice may fail silently if all values of one variable are similar ( constant ) which may occurs when applying one hot encoding with a large number of splits. Parameters: Name Type Description Default inputs IndicesInput The fairness problem to study. required n number of sample used to compute the sobol indices 1000 N number of sample used to compute marginals None Returns: Type Description IndicesOutput IndicesOutput object, containing the CVM indices, one line per variable group IndicesOutput and one column for each index. Source code in deel\\fairsense\\indices\\sobol.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def sobol_indices ( inputs : IndicesInput , n = 1000 , N = None ) -> IndicesOutput : \"\"\" Compute all sobol indices for all variables Warning: this indice may fail silently if all values of one variable are similar ( constant ) which may occurs when applying one hot encoding with a large number of splits. Args: inputs (IndicesInput): The fairness problem to study. n: number of sample used to compute the sobol indices N: number of sample used to compute marginals Returns: IndicesOutput object, containing the CVM indices, one line per variable group and one column for each index. \"\"\" x = inputs . x cov = ( x + np . random . normal ( scale = 1e-5 , size = x . shape )) . cov () orig_cols = [] for group in inputs . variable_groups : orig_cols += group f_inv = _compute_marginal_inv_cumul_dist ( x [ orig_cols ] . values , N ) sobol_table = [] for i in range ( len ( inputs . variable_groups )): sobol_table . append ( _sobol_indices_at_i ( inputs . compute_objective , i , inputs . variable_groups , n , cov , f_inv ) ) sobol_table = np . vstack ( sobol_table ) sobol_table [:, 2 :] = np . roll ( sobol_table [:, 2 :], - 1 , axis = 0 ) return IndicesOutput ( pd . DataFrame ( data = sobol_table , index = inputs . merged_groups , columns = [ \"S\" , \"ST\" , \"S_ind\" , \"ST_ind\" ], ) ) cvm_indices ( index_input ) \u00b6 Compute the CVM indices of a fairness problem. Set FairnessProblem.result as a Dataframe containing the indices. Warning this indice may fail silently if all values of one variable are similar ( constant ) which may occurs when applying one hot encoding with a large number of splits. It may also yield erroneous results when used without enough data. Which might occur when used with confidence intervals. Parameters: Name Type Description Default index_input IndicesInput The fairness problem to study. required Returns: Type Description IndicesOutput IndicesOutput object, containing the CVM indices, one line per variable group IndicesOutput and one column for each index. Source code in deel\\fairsense\\indices\\cvm.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def cvm_indices ( index_input : IndicesInput ) -> IndicesOutput : \"\"\"Compute the CVM indices of a fairness problem. Set FairnessProblem.result as a Dataframe containing the indices. Warning: this indice may fail silently if all values of one variable are similar ( constant ) which may occurs when applying one hot encoding with a large number of splits. It may also yield erroneous results when used without enough data. Which might occur when used with confidence intervals. Args: index_input (IndicesInput): The fairness problem to study. Returns: IndicesOutput object, containing the CVM indices, one line per variable group and one column for each index. \"\"\" # __check_arg_cvm(index_input, cols) df = pd . DataFrame ( index_input . x , columns = index_input . x . columns ) df [ \"outputs\" ] = pd . DataFrame ( index_input . compute_objective ()) return IndicesOutput ( _analyze ( df , \"outputs\" , cols = index_input . variable_groups )) with_confidence_intervals ( n_splits = 31 , shuffle = False , random_state = None ) \u00b6 Function decorator that allows to compute confidence intervals using the naive method. The input data is split in n_splits and for each split indices are computed. Warnings No correction if applied on the output (small number of split will lead to overconfident intervals and a large number of split will lead to a large variance due to the lack of data). This function must be applied on one of the indices computation function from the indices module. Parameters: Name Type Description Default n_splits positive integer : number of split. 31 shuffle Whether to shuffle the data before splitting into batches. Note that the samples within each split will not be shuffled. False random_state When shuffle is True, random_state affects the ordering of the indices, which controls the randomness of each fold. Otherwise, this parameter has no effect. Pass an int for reproducible output across multiple function calls. None Returns: Type Description the original indice computation function enriched to compute confidence intervals. Source code in deel\\fairsense\\indices\\confidence_intervals.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def with_confidence_intervals ( n_splits = 31 , shuffle = False , random_state = None ): \"\"\" Function decorator that allows to compute confidence intervals using the naive method. The input data is split in n_splits and for each split indices are computed. Warnings: No correction if applied on the output (small number of split will lead to overconfident intervals and a large number of split will lead to a large variance due to the lack of data). This function must be applied on one of the indices computation function from the indices module. Args: n_splits: positive integer : number of split. shuffle: Whether to shuffle the data before splitting into batches. Note that the samples within each split will not be shuffled. random_state: When `shuffle` is True, `random_state` affects the ordering of the indices, which controls the randomness of each fold. Otherwise, this parameter has no effect. Pass an int for reproducible output across multiple function calls. Returns: the original indice computation function enriched to compute confidence intervals. \"\"\" kf = KFold ( n_splits , shuffle = shuffle , random_state = random_state ) def confidence_computation_fct ( function ): def call_function ( inputs : IndicesInput , * args , ** kwargs ): # get full inputs x = inputs . x y = inputs . y_true fold_results = [] # repeat indices computation on each fold for _ , split in tqdm ( kf . split ( x , y ), total = n_splits , ncols = 80 ): # build input for the fold x_fold = x . iloc [ split ] y_fold = y . iloc [ split ] if y is not None else None fold_inputs = IndicesInput ( model = inputs . model , x = x_fold , y_true = y_fold , variable_groups = inputs . variable_groups , objective = inputs . objective , ) # compute the result for the fold fold_results . append ( function ( fold_inputs , * args , ** kwargs )) # merge results to compute values and confidence intervals fvalues = [ f . values for f in fold_results ] runs = pd . concat ( fvalues ) return IndicesOutput ( runs ) return call_function return confidence_computation_fct","title":"indices module"},{"location":"api/indices/#statistical-parity-disparate-impact-demographic-parity","text":"The rates of value-1 predictions from the different groups must be equal. Independence between the predictor and the protected variable. S binary \\(P(f(X)=1|S=0) = P(f(X)=1|S=1)\\) S continuous or discrete \\(P(f(X)=1|S) = P(f(X)=1)\\)","title":"Statistical Parity - Disparate Impact - Demographic Parity"},{"location":"api/indices/#avoiding-of-disparate-treatment","text":"The probability that an input leads to prediction 1 should be equal regardless of the value of the sensitive variable. S binary \\(P(f(X)=1|X_S=x,S=0) = P(f(X)=1|X_S=x,S=1)\\) where \\(X_S\\) represents \\(X\\) without the sensitive variable.","title":"Avoiding of Disparate Treatment"},{"location":"api/indices/#equality-of-odds","text":"The rates of true and false predictions from the different groups must be equal. Independence between the error of the model and the protected variable. S binary \\(P(f(X)=1|Y=i,S=0) = P(f(X)=1|Y=i,S=1) ,i=0,1\\) S general \\(P(f(X)=1|Y=i,S) = P(f(X)=1|Y=i) ,i=0,1\\)","title":"Equality of Odds"},{"location":"api/indices/#avoiding-of-disparate-mistreatment","text":"The probability that a prediction is false should be equal regardless of the value of the sensitive variable. - S binary \\(P(f(X)\\ne Y|S=1) = P(f(X)\\ne Y|S=0)\\)","title":"Avoiding of Disparate Mistreatment"},{"location":"api/indices/#global-sensitivity-analysis","text":"GSA is used for quantifying the influence of a set of features on the outcome. Sobol' indices are based on correlations and need access to the function while CVM' indices are based on rank and need only a sample of evaluations. Sobol' indices 4 indices that quantify how much of the output variance can be explained by the variance of Xi. Correlation Between Variables Joined Contributions \\(Sob_i\\) \u2714\ufe0f \u274c \\(SobT_i\\) \u2714\ufe0f \u2714\ufe0f \\(Sob_i^{ind}\\) \u274c \u274c \\(SobT_i^{ind}\\) \u274c \u2714\ufe0f Cramer-Von Mises' indices The 2 CVM' indices is an extension of the Sobol\u2019 indices to quantify more than just the second-order influence of the inputs on the output. For further details about GSA in Fairness","title":"Global Sensitivity Analysis"},{"location":"api/indices/#case-of-use-recap","text":"Disparate Impact Avoiding Disparate Treatment Equality Odds Avoiding Disparate Mistreatment Sobol' indices Cramer-Von Mises' indices S binary \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f S discrete \u2714\ufe0f \u274c \u2714\ufe0f \u274c \u2714\ufe0f \u2714\ufe0f S continuous \u2714\ufe0f \u274c \u2714\ufe0f \u274c \u2714\ufe0f \u2714\ufe0f","title":"Case-of-use Recap"},{"location":"api/indices/#deel.fairsense.indices.standard_metrics.disparate_impact","text":"Compute the disparate impact. Warning disparate impact/equality of odds can only be computed on classification problems, and on categorical variables. Continuous variables are dropped and output replaced by np.nan Note When applied with target=classification_error this function compute the equality of odds. Parameters: Name Type Description Default index_input IndicesInput The fairness problem to study. required group_reduction the method used to compute the indices for a group of variables. By default the average of the values of each groups is applied. np.mean Returns: Type Description IndicesOutput IndicesOutput object, containing the CVM indices, one line per variable group IndicesOutput and one column for each index. Source code in deel\\fairsense\\indices\\standard_metrics.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def disparate_impact ( index_input : IndicesInput , group_reduction = np . mean ) -> IndicesOutput : \"\"\" Compute the disparate impact. Warning: disparate impact/equality of odds can only be computed on classification problems, and on categorical variables. Continuous variables are dropped and output replaced by `np.nan` Note: When applied with `target=classification_error` this function compute the equality of odds. Args: index_input (IndicesInput): The fairness problem to study. group_reduction: the method used to compute the indices for a group of variables. By default the average of the values of each groups is applied. Returns: IndicesOutput object, containing the CVM indices, one line per variable group and one column for each index. \"\"\" df = index_input . x y = index_input . compute_objective () df [ \"outputs\" ] = y . values if hasattr ( y , \"values\" ) else y dis = [] for group in index_input . variable_groups : group_output = [] for var in group : group_output . append ( _disparate_impact_single_variable ( df , var )) dis . append ( group_reduction ( group_output )) data = np . expand_dims ( np . array ( dis ), axis =- 1 ) index = index_input . merged_groups results = pd . DataFrame ( data = data , columns = [ \"DI\" ], index = index ) return IndicesOutput ( results )","title":"disparate_impact()"},{"location":"api/indices/#deel.fairsense.indices.sobol.sobol_indices","text":"Compute all sobol indices for all variables Warning this indice may fail silently if all values of one variable are similar ( constant ) which may occurs when applying one hot encoding with a large number of splits. Parameters: Name Type Description Default inputs IndicesInput The fairness problem to study. required n number of sample used to compute the sobol indices 1000 N number of sample used to compute marginals None Returns: Type Description IndicesOutput IndicesOutput object, containing the CVM indices, one line per variable group IndicesOutput and one column for each index. Source code in deel\\fairsense\\indices\\sobol.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def sobol_indices ( inputs : IndicesInput , n = 1000 , N = None ) -> IndicesOutput : \"\"\" Compute all sobol indices for all variables Warning: this indice may fail silently if all values of one variable are similar ( constant ) which may occurs when applying one hot encoding with a large number of splits. Args: inputs (IndicesInput): The fairness problem to study. n: number of sample used to compute the sobol indices N: number of sample used to compute marginals Returns: IndicesOutput object, containing the CVM indices, one line per variable group and one column for each index. \"\"\" x = inputs . x cov = ( x + np . random . normal ( scale = 1e-5 , size = x . shape )) . cov () orig_cols = [] for group in inputs . variable_groups : orig_cols += group f_inv = _compute_marginal_inv_cumul_dist ( x [ orig_cols ] . values , N ) sobol_table = [] for i in range ( len ( inputs . variable_groups )): sobol_table . append ( _sobol_indices_at_i ( inputs . compute_objective , i , inputs . variable_groups , n , cov , f_inv ) ) sobol_table = np . vstack ( sobol_table ) sobol_table [:, 2 :] = np . roll ( sobol_table [:, 2 :], - 1 , axis = 0 ) return IndicesOutput ( pd . DataFrame ( data = sobol_table , index = inputs . merged_groups , columns = [ \"S\" , \"ST\" , \"S_ind\" , \"ST_ind\" ], ) )","title":"sobol_indices()"},{"location":"api/indices/#deel.fairsense.indices.cvm.cvm_indices","text":"Compute the CVM indices of a fairness problem. Set FairnessProblem.result as a Dataframe containing the indices. Warning this indice may fail silently if all values of one variable are similar ( constant ) which may occurs when applying one hot encoding with a large number of splits. It may also yield erroneous results when used without enough data. Which might occur when used with confidence intervals. Parameters: Name Type Description Default index_input IndicesInput The fairness problem to study. required Returns: Type Description IndicesOutput IndicesOutput object, containing the CVM indices, one line per variable group IndicesOutput and one column for each index. Source code in deel\\fairsense\\indices\\cvm.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def cvm_indices ( index_input : IndicesInput ) -> IndicesOutput : \"\"\"Compute the CVM indices of a fairness problem. Set FairnessProblem.result as a Dataframe containing the indices. Warning: this indice may fail silently if all values of one variable are similar ( constant ) which may occurs when applying one hot encoding with a large number of splits. It may also yield erroneous results when used without enough data. Which might occur when used with confidence intervals. Args: index_input (IndicesInput): The fairness problem to study. Returns: IndicesOutput object, containing the CVM indices, one line per variable group and one column for each index. \"\"\" # __check_arg_cvm(index_input, cols) df = pd . DataFrame ( index_input . x , columns = index_input . x . columns ) df [ \"outputs\" ] = pd . DataFrame ( index_input . compute_objective ()) return IndicesOutput ( _analyze ( df , \"outputs\" , cols = index_input . variable_groups ))","title":"cvm_indices()"},{"location":"api/indices/#deel.fairsense.indices.confidence_intervals.with_confidence_intervals","text":"Function decorator that allows to compute confidence intervals using the naive method. The input data is split in n_splits and for each split indices are computed. Warnings No correction if applied on the output (small number of split will lead to overconfident intervals and a large number of split will lead to a large variance due to the lack of data). This function must be applied on one of the indices computation function from the indices module. Parameters: Name Type Description Default n_splits positive integer : number of split. 31 shuffle Whether to shuffle the data before splitting into batches. Note that the samples within each split will not be shuffled. False random_state When shuffle is True, random_state affects the ordering of the indices, which controls the randomness of each fold. Otherwise, this parameter has no effect. Pass an int for reproducible output across multiple function calls. None Returns: Type Description the original indice computation function enriched to compute confidence intervals. Source code in deel\\fairsense\\indices\\confidence_intervals.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def with_confidence_intervals ( n_splits = 31 , shuffle = False , random_state = None ): \"\"\" Function decorator that allows to compute confidence intervals using the naive method. The input data is split in n_splits and for each split indices are computed. Warnings: No correction if applied on the output (small number of split will lead to overconfident intervals and a large number of split will lead to a large variance due to the lack of data). This function must be applied on one of the indices computation function from the indices module. Args: n_splits: positive integer : number of split. shuffle: Whether to shuffle the data before splitting into batches. Note that the samples within each split will not be shuffled. random_state: When `shuffle` is True, `random_state` affects the ordering of the indices, which controls the randomness of each fold. Otherwise, this parameter has no effect. Pass an int for reproducible output across multiple function calls. Returns: the original indice computation function enriched to compute confidence intervals. \"\"\" kf = KFold ( n_splits , shuffle = shuffle , random_state = random_state ) def confidence_computation_fct ( function ): def call_function ( inputs : IndicesInput , * args , ** kwargs ): # get full inputs x = inputs . x y = inputs . y_true fold_results = [] # repeat indices computation on each fold for _ , split in tqdm ( kf . split ( x , y ), total = n_splits , ncols = 80 ): # build input for the fold x_fold = x . iloc [ split ] y_fold = y . iloc [ split ] if y is not None else None fold_inputs = IndicesInput ( model = inputs . model , x = x_fold , y_true = y_fold , variable_groups = inputs . variable_groups , objective = inputs . objective , ) # compute the result for the fold fold_results . append ( function ( fold_inputs , * args , ** kwargs )) # merge results to compute values and confidence intervals fvalues = [ f . values for f in fold_results ] runs = pd . concat ( fvalues ) return IndicesOutput ( runs ) return call_function return confidence_computation_fct","title":"with_confidence_intervals()"},{"location":"api/utils_dataclasses/","text":"IndicesInput \u00b6 Source code in deel\\fairsense\\utils\\dataclasses.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class IndicesInput : def __init__ ( self , model : Optional [ Callable ] = None , x : Optional [ DataFrame ] = None , y_true : Optional [ DataFrame ] = None , objective : Callable = None , variable_groups : List [ List [ str ]] = None , ): \"\"\" Build an IndiceInput object. Args: model: function that can be applied on x, that return a series with same shape as y_true. x: a dataframe containing the samples to analyse. y_true: a dataframe containing the labels of the samples (in the same order) objective: one of the target from the utils.fairness_objective module. variable_groups: list of list, containing the name of the columns that should be grouped. \"\"\" self . model = model self . _variable_groups = variable_groups self . _x = x self . _x . columns = [ str ( c ) for c in x . columns ] self . _y_true = y_true self . objective = objective @property def x ( self ): # indice_input.x returns a copy of the data return self . _x . copy () def compute_objective ( self , x = None ): \"\"\" Compute the objective, using available data. When objective is y_true, y_true is returned, when objective is y_pred, the model is applied on x, and other objective compute the difference between y_true and y_pred. Args: x: the sample to compute the objective on. When None, `self.x` is used. Returns: the value of the objective. \"\"\" return self . objective ( self , x ) @property def y_true ( self ): return self . _y_true . copy () if self . _y_true is not None else None @y_true . setter def y_true ( self , _y ): # this setter ensures that y_true is a dataframe and not a series if _y is None : self . _y_true = None else : if len ( _y . shape ) < 2 : _y = np . expand_dims ( _y , - 1 ) self . _y_true = DataFrame ( _y , columns = [ \"outputs\" ]) elif isinstance ( _y , DataFrame ): self . _y_true = _y else : self . _y_true = DataFrame ( _y , columns = [ \"outputs\" ]) @property def variable_groups ( self ): if self . _variable_groups is None : return [[ str ( var )] for var in self . _x . columns ] else : return self . _variable_groups @property def merged_groups ( self ): return [ x [ 0 ] . split ( \"=\" )[ 0 ] for x in self . variable_groups ] __init__ ( model = None , x = None , y_true = None , objective = None , variable_groups = None ) \u00b6 Build an IndiceInput object. Parameters: Name Type Description Default model Optional [ Callable ] function that can be applied on x, that return a series with None x Optional [ DataFrame ] a dataframe containing the samples to analyse. None y_true Optional [ DataFrame ] a dataframe containing the labels of the samples (in the same order) None objective Callable one of the target from the utils.fairness_objective module. None variable_groups List [ List [ str ]] list of list, containing the name of the columns that should be grouped. None Source code in deel\\fairsense\\utils\\dataclasses.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , model : Optional [ Callable ] = None , x : Optional [ DataFrame ] = None , y_true : Optional [ DataFrame ] = None , objective : Callable = None , variable_groups : List [ List [ str ]] = None , ): \"\"\" Build an IndiceInput object. Args: model: function that can be applied on x, that return a series with same shape as y_true. x: a dataframe containing the samples to analyse. y_true: a dataframe containing the labels of the samples (in the same order) objective: one of the target from the utils.fairness_objective module. variable_groups: list of list, containing the name of the columns that should be grouped. \"\"\" self . model = model self . _variable_groups = variable_groups self . _x = x self . _x . columns = [ str ( c ) for c in x . columns ] self . _y_true = y_true self . objective = objective compute_objective ( x = None ) \u00b6 Compute the objective, using available data. When objective is y_true, y_true is returned, when objective is y_pred, the model is applied on x, and other objective compute the difference between y_true and y_pred. Parameters: Name Type Description Default x the sample to compute the objective on. When None, self.x is used. None Returns: Type Description the value of the objective. Source code in deel\\fairsense\\utils\\dataclasses.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def compute_objective ( self , x = None ): \"\"\" Compute the objective, using available data. When objective is y_true, y_true is returned, when objective is y_pred, the model is applied on x, and other objective compute the difference between y_true and y_pred. Args: x: the sample to compute the objective on. When None, `self.x` is used. Returns: the value of the objective. \"\"\" return self . objective ( self , x ) IndicesOutput \u00b6 Source code in deel\\fairsense\\utils\\dataclasses.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class IndicesOutput : def __init__ ( self , values : DataFrame ): \"\"\" Encapsulate the results of the analysis. Every function from the indices module returns an object of this type. This object override the `+` operator allorw to combine result more easily. Args: values: a DataFrame containing the values of the indices. When confidence intervals are enabled this dataframe contains the results of each split. \"\"\" self . runs : DataFrame = values # 2D dataframe: lines=variable groups @property def values ( self ): return self . runs . groupby ( level = 0 ) . median () . clip ( 0.0 , 1.0 ) def __add__ ( self , other ): # indices must be computed on same groups assert other . runs . shape [ 0 ] == self . runs . shape [ 0 ] new_values = self . runs . copy () new_values [ other . runs . columns ] = other . runs return IndicesOutput ( new_values ) __init__ ( values ) \u00b6 Encapsulate the results of the analysis. Every function from the indices module returns an object of this type. This object override the + operator allorw to combine result more easily. Parameters: Name Type Description Default values DataFrame a DataFrame containing the values of the indices. When confidence required Source code in deel\\fairsense\\utils\\dataclasses.py 113 114 115 116 117 118 119 120 121 122 123 def __init__ ( self , values : DataFrame ): \"\"\" Encapsulate the results of the analysis. Every function from the indices module returns an object of this type. This object override the `+` operator allorw to combine result more easily. Args: values: a DataFrame containing the values of the indices. When confidence intervals are enabled this dataframe contains the results of each split. \"\"\" self . runs : DataFrame = values # 2D dataframe: lines=variable groups","title":"dataclasses module"},{"location":"api/utils_dataclasses/#deel.fairsense.utils.dataclasses.IndicesInput","text":"Source code in deel\\fairsense\\utils\\dataclasses.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class IndicesInput : def __init__ ( self , model : Optional [ Callable ] = None , x : Optional [ DataFrame ] = None , y_true : Optional [ DataFrame ] = None , objective : Callable = None , variable_groups : List [ List [ str ]] = None , ): \"\"\" Build an IndiceInput object. Args: model: function that can be applied on x, that return a series with same shape as y_true. x: a dataframe containing the samples to analyse. y_true: a dataframe containing the labels of the samples (in the same order) objective: one of the target from the utils.fairness_objective module. variable_groups: list of list, containing the name of the columns that should be grouped. \"\"\" self . model = model self . _variable_groups = variable_groups self . _x = x self . _x . columns = [ str ( c ) for c in x . columns ] self . _y_true = y_true self . objective = objective @property def x ( self ): # indice_input.x returns a copy of the data return self . _x . copy () def compute_objective ( self , x = None ): \"\"\" Compute the objective, using available data. When objective is y_true, y_true is returned, when objective is y_pred, the model is applied on x, and other objective compute the difference between y_true and y_pred. Args: x: the sample to compute the objective on. When None, `self.x` is used. Returns: the value of the objective. \"\"\" return self . objective ( self , x ) @property def y_true ( self ): return self . _y_true . copy () if self . _y_true is not None else None @y_true . setter def y_true ( self , _y ): # this setter ensures that y_true is a dataframe and not a series if _y is None : self . _y_true = None else : if len ( _y . shape ) < 2 : _y = np . expand_dims ( _y , - 1 ) self . _y_true = DataFrame ( _y , columns = [ \"outputs\" ]) elif isinstance ( _y , DataFrame ): self . _y_true = _y else : self . _y_true = DataFrame ( _y , columns = [ \"outputs\" ]) @property def variable_groups ( self ): if self . _variable_groups is None : return [[ str ( var )] for var in self . _x . columns ] else : return self . _variable_groups @property def merged_groups ( self ): return [ x [ 0 ] . split ( \"=\" )[ 0 ] for x in self . variable_groups ]","title":"IndicesInput"},{"location":"api/utils_dataclasses/#deel.fairsense.utils.dataclasses.IndicesInput.__init__","text":"Build an IndiceInput object. Parameters: Name Type Description Default model Optional [ Callable ] function that can be applied on x, that return a series with None x Optional [ DataFrame ] a dataframe containing the samples to analyse. None y_true Optional [ DataFrame ] a dataframe containing the labels of the samples (in the same order) None objective Callable one of the target from the utils.fairness_objective module. None variable_groups List [ List [ str ]] list of list, containing the name of the columns that should be grouped. None Source code in deel\\fairsense\\utils\\dataclasses.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , model : Optional [ Callable ] = None , x : Optional [ DataFrame ] = None , y_true : Optional [ DataFrame ] = None , objective : Callable = None , variable_groups : List [ List [ str ]] = None , ): \"\"\" Build an IndiceInput object. Args: model: function that can be applied on x, that return a series with same shape as y_true. x: a dataframe containing the samples to analyse. y_true: a dataframe containing the labels of the samples (in the same order) objective: one of the target from the utils.fairness_objective module. variable_groups: list of list, containing the name of the columns that should be grouped. \"\"\" self . model = model self . _variable_groups = variable_groups self . _x = x self . _x . columns = [ str ( c ) for c in x . columns ] self . _y_true = y_true self . objective = objective","title":"__init__()"},{"location":"api/utils_dataclasses/#deel.fairsense.utils.dataclasses.IndicesInput.compute_objective","text":"Compute the objective, using available data. When objective is y_true, y_true is returned, when objective is y_pred, the model is applied on x, and other objective compute the difference between y_true and y_pred. Parameters: Name Type Description Default x the sample to compute the objective on. When None, self.x is used. None Returns: Type Description the value of the objective. Source code in deel\\fairsense\\utils\\dataclasses.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def compute_objective ( self , x = None ): \"\"\" Compute the objective, using available data. When objective is y_true, y_true is returned, when objective is y_pred, the model is applied on x, and other objective compute the difference between y_true and y_pred. Args: x: the sample to compute the objective on. When None, `self.x` is used. Returns: the value of the objective. \"\"\" return self . objective ( self , x )","title":"compute_objective()"},{"location":"api/utils_dataclasses/#deel.fairsense.utils.dataclasses.IndicesOutput","text":"Source code in deel\\fairsense\\utils\\dataclasses.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class IndicesOutput : def __init__ ( self , values : DataFrame ): \"\"\" Encapsulate the results of the analysis. Every function from the indices module returns an object of this type. This object override the `+` operator allorw to combine result more easily. Args: values: a DataFrame containing the values of the indices. When confidence intervals are enabled this dataframe contains the results of each split. \"\"\" self . runs : DataFrame = values # 2D dataframe: lines=variable groups @property def values ( self ): return self . runs . groupby ( level = 0 ) . median () . clip ( 0.0 , 1.0 ) def __add__ ( self , other ): # indices must be computed on same groups assert other . runs . shape [ 0 ] == self . runs . shape [ 0 ] new_values = self . runs . copy () new_values [ other . runs . columns ] = other . runs return IndicesOutput ( new_values )","title":"IndicesOutput"},{"location":"api/utils_dataclasses/#deel.fairsense.utils.dataclasses.IndicesOutput.__init__","text":"Encapsulate the results of the analysis. Every function from the indices module returns an object of this type. This object override the + operator allorw to combine result more easily. Parameters: Name Type Description Default values DataFrame a DataFrame containing the values of the indices. When confidence required Source code in deel\\fairsense\\utils\\dataclasses.py 113 114 115 116 117 118 119 120 121 122 123 def __init__ ( self , values : DataFrame ): \"\"\" Encapsulate the results of the analysis. Every function from the indices module returns an object of this type. This object override the `+` operator allorw to combine result more easily. Args: values: a DataFrame containing the values of the indices. When confidence intervals are enabled this dataframe contains the results of each split. \"\"\" self . runs : DataFrame = values # 2D dataframe: lines=variable groups","title":"__init__()"},{"location":"api/utils_fairness_objective/","text":"This module contains the fairness objectives. These functions won't be called as is, as these will be passed to a IndicesInput.objective . classification_error ( self , x = None ) \u00b6 Evaluate the fairness of the model's errors over the dataset. This allow to check if the model errors are due to the presence of a sensitive attribute. The error is computed for classification by checking if the model output is equal to y_true given in the IndicesInput . Source code in deel\\fairsense\\utils\\fairness_objective.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def classification_error ( self : IndicesInput , x = None ): \"\"\" Evaluate the fairness of the model's errors over the dataset. This allow to check if the model errors are due to the presence of a sensitive attribute. The error is computed for classification by checking if the model output is equal to `y_true` given in the `IndicesInput`. \"\"\" if x is not None : raise RuntimeError ( \"this target can only be used with x=None\" ) y_pred = self . model ( self . x ) # if len(y_pred.shape) < 2: # y_pred = np.expand_dims(y_pred, -1) return np . not_equal ( self . y_true , y_pred ) . astype ( float ) squared_error ( self , x = None ) \u00b6 Evaluate the fairness of the model's errors over the dataset. This allow to check if the model errors are due to the presence of a sensitive attribute. The error is computed for regression by measuring the squared error between the model output and y_true given in the IndicesInput . Source code in deel\\fairsense\\utils\\fairness_objective.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def squared_error ( self : IndicesInput , x = None ): \"\"\" Evaluate the fairness of the model's errors over the dataset. This allow to check if the model errors are due to the presence of a sensitive attribute. The error is computed for regression by measuring the squared error between the model output and `y_true` given in the `IndicesInput`. \"\"\" if x is not None : raise RuntimeError ( \"this target can only be used with x=None\" ) y_pred = self . model ( self . x ) if len ( y_pred . shape ) < 2 : y_pred = np . expand_dims ( y_pred , - 1 ) return np . square ( self . y_true - y_pred ) y_pred ( self , x = None ) \u00b6 Evaluate the fairness of the model's predictions over the dataset. This allow to check if the model gives biased decisions. Source code in deel\\fairsense\\utils\\fairness_objective.py 42 43 44 45 46 47 def y_pred ( self : IndicesInput , x = None ): \"\"\" Evaluate the fairness of the model's predictions over the dataset. This allow to check if the model gives biased decisions. \"\"\" return np . float32 ( self . model ( x if x is not None else self . x )) y_true ( self , x = None ) \u00b6 Evaluate the intrinsic fairness of the dataset. This allow to check if the data used for training is biased. Source code in deel\\fairsense\\utils\\fairness_objective.py 32 33 34 35 36 37 38 39 def y_true ( self : IndicesInput , x = None ): \"\"\" Evaluate the intrinsic fairness of the dataset. This allow to check if the data used for training is biased. \"\"\" if x is not None : raise RuntimeError ( \"this target can only be used with x=None\" ) return self . y_true","title":"fairness_objective module"},{"location":"api/utils_fairness_objective/#deel.fairsense.utils.fairness_objective.classification_error","text":"Evaluate the fairness of the model's errors over the dataset. This allow to check if the model errors are due to the presence of a sensitive attribute. The error is computed for classification by checking if the model output is equal to y_true given in the IndicesInput . Source code in deel\\fairsense\\utils\\fairness_objective.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def classification_error ( self : IndicesInput , x = None ): \"\"\" Evaluate the fairness of the model's errors over the dataset. This allow to check if the model errors are due to the presence of a sensitive attribute. The error is computed for classification by checking if the model output is equal to `y_true` given in the `IndicesInput`. \"\"\" if x is not None : raise RuntimeError ( \"this target can only be used with x=None\" ) y_pred = self . model ( self . x ) # if len(y_pred.shape) < 2: # y_pred = np.expand_dims(y_pred, -1) return np . not_equal ( self . y_true , y_pred ) . astype ( float )","title":"classification_error()"},{"location":"api/utils_fairness_objective/#deel.fairsense.utils.fairness_objective.squared_error","text":"Evaluate the fairness of the model's errors over the dataset. This allow to check if the model errors are due to the presence of a sensitive attribute. The error is computed for regression by measuring the squared error between the model output and y_true given in the IndicesInput . Source code in deel\\fairsense\\utils\\fairness_objective.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def squared_error ( self : IndicesInput , x = None ): \"\"\" Evaluate the fairness of the model's errors over the dataset. This allow to check if the model errors are due to the presence of a sensitive attribute. The error is computed for regression by measuring the squared error between the model output and `y_true` given in the `IndicesInput`. \"\"\" if x is not None : raise RuntimeError ( \"this target can only be used with x=None\" ) y_pred = self . model ( self . x ) if len ( y_pred . shape ) < 2 : y_pred = np . expand_dims ( y_pred , - 1 ) return np . square ( self . y_true - y_pred )","title":"squared_error()"},{"location":"api/utils_fairness_objective/#deel.fairsense.utils.fairness_objective.y_pred","text":"Evaluate the fairness of the model's predictions over the dataset. This allow to check if the model gives biased decisions. Source code in deel\\fairsense\\utils\\fairness_objective.py 42 43 44 45 46 47 def y_pred ( self : IndicesInput , x = None ): \"\"\" Evaluate the fairness of the model's predictions over the dataset. This allow to check if the model gives biased decisions. \"\"\" return np . float32 ( self . model ( x if x is not None else self . x ))","title":"y_pred()"},{"location":"api/utils_fairness_objective/#deel.fairsense.utils.fairness_objective.y_true","text":"Evaluate the intrinsic fairness of the dataset. This allow to check if the data used for training is biased. Source code in deel\\fairsense\\utils\\fairness_objective.py 32 33 34 35 36 37 38 39 def y_true ( self : IndicesInput , x = None ): \"\"\" Evaluate the intrinsic fairness of the dataset. This allow to check if the data used for training is biased. \"\"\" if x is not None : raise RuntimeError ( \"this target can only be used with x=None\" ) return self . y_true","title":"y_true()"},{"location":"api/visualization/","text":"cat_plot ( indices , plot_per = 'variable' , kind = 'bar' , col_wrap = None , ** kwargs ) \u00b6 Uses the seaborn.catplot _ to plot the indices. Parameters: Name Type Description Default indices IndicesOutput computed indices required plot_per str can be either variable or index , when set to variable there is one graph per variable, each graph showing the values of all indices. Respectively setting to index will build one graph per index, each showing the values for all variable. 'variable' kind str kind of visualization to produce, can be one of strip , swarm , box , violin , boxen , point , bar . 'bar' col_wrap Optional(int \u201cWrap\u201d the column variable at this width, so that the column facets span multiple rows. None **kwargs extra arguments passed to seaborn.catplot . {} Returns: Type Description a matplotlib axes object Source code in deel\\fairsense\\visualization\\plots.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def cat_plot ( indices : IndicesOutput , plot_per = \"variable\" , kind = \"bar\" , col_wrap = None , ** kwargs ): \"\"\" Uses the `seaborn.catplot`_ to plot the indices. Args: indices (IndicesOutput): computed indices plot_per (str): can be either `variable` or `index`, when set to `variable` there is one graph per variable, each graph showing the values of all indices. Respectively setting to `index` will build one graph per index, each showing the values for all variable. kind (str): kind of visualization to produce, can be one of `strip`, `swarm`, `box`, `violin`, `boxen`, `point`, `bar`. col_wrap (Optional(int)): \u201cWrap\u201d the column variable at this width, so that the column facets span multiple rows. **kwargs: extra arguments passed to [seaborn.catplot]( https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot). Returns: a matplotlib axes object \"\"\" assert plot_per . lower () . strip () in { \"variable\" , \"index\" , \"indices\" } indices_names = indices . values . columns variable_names = indices . values . index data = indices . runs . stack () . reset_index () data . rename ( columns = { \"level_0\" : \"variable\" , \"level_1\" : \"index\" , 0 : \"value\" }, inplace = True ) if plot_per == \"variable\" : col = \"variable\" x = \"index\" order = indices_names else : col = \"index\" x = \"variable\" order = variable_names ax = sns . catplot ( data = data , x = x , y = \"value\" , col = col , kind = kind , col_wrap = col_wrap , order = order , ** kwargs ) ax . set ( ylim = ( 0.0 , 1.0 )) ax . set_xticklabels ( rotation = 45 , horizontalalignment = \"right\" ) return ax format_with_intervals ( indices_outputs , quantile = 0.05 ) \u00b6 Pretty print the indices table with confidence intervals. Note that the intervals are displayed even if the indices are computed without confidence intervals. See :mod: fairsense.indices.confidence_intervals for more information. Parameters: Name Type Description Default indices_outputs IndicesOutput computed indices quantile (float): quantile used to compute confidence intervals. Values must be in [0., 0.5]. required the table with indices properly displayed. Note that the table values Type Description are now string and not float. Source code in deel\\fairsense\\visualization\\text.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def format_with_intervals ( indices_outputs : IndicesOutput , quantile : float = 0.05 ): \"\"\" Pretty print the indices table with confidence intervals. Note that the intervals are displayed even if the indices are computed without confidence intervals. See :mod:`fairsense.indices.confidence_intervals` for more information. Args: indices_outputs (IndicesOutput): computed indices quantile (float): quantile used to compute confidence intervals. Values must be in [0., 0.5]. Returns: the table with indices properly displayed. Note that the table values are now string and not float. \"\"\" means = indices_outputs . runs . groupby ( level = 0 ) . median () . clip ( 0.0 , 1.0 ) low = ( indices_outputs . runs . groupby ( level = 0 ) . aggregate ( partial ( np . quantile , q = quantile )) . clip ( 0.0 , 1.0 ) ) high = ( indices_outputs . runs . groupby ( level = 0 ) . aggregate ( partial ( np . quantile , q = 1 - quantile )) . clip ( 0.0 , 1.0 ) ) table = means . copy () for index in means . columns : table [ index ] = np . vectorize ( lambda index_val , index_inf_val , index_sup_val : \" %.2f [ %.2f , %.2f ]\" % ( index_val , index_inf_val , index_sup_val ) )( means [ index ], low [ index ], high [ index ]) return table","title":"visualization module"},{"location":"api/visualization/#deel.fairsense.visualization.plots.cat_plot","text":"Uses the seaborn.catplot _ to plot the indices. Parameters: Name Type Description Default indices IndicesOutput computed indices required plot_per str can be either variable or index , when set to variable there is one graph per variable, each graph showing the values of all indices. Respectively setting to index will build one graph per index, each showing the values for all variable. 'variable' kind str kind of visualization to produce, can be one of strip , swarm , box , violin , boxen , point , bar . 'bar' col_wrap Optional(int \u201cWrap\u201d the column variable at this width, so that the column facets span multiple rows. None **kwargs extra arguments passed to seaborn.catplot . {} Returns: Type Description a matplotlib axes object Source code in deel\\fairsense\\visualization\\plots.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def cat_plot ( indices : IndicesOutput , plot_per = \"variable\" , kind = \"bar\" , col_wrap = None , ** kwargs ): \"\"\" Uses the `seaborn.catplot`_ to plot the indices. Args: indices (IndicesOutput): computed indices plot_per (str): can be either `variable` or `index`, when set to `variable` there is one graph per variable, each graph showing the values of all indices. Respectively setting to `index` will build one graph per index, each showing the values for all variable. kind (str): kind of visualization to produce, can be one of `strip`, `swarm`, `box`, `violin`, `boxen`, `point`, `bar`. col_wrap (Optional(int)): \u201cWrap\u201d the column variable at this width, so that the column facets span multiple rows. **kwargs: extra arguments passed to [seaborn.catplot]( https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot). Returns: a matplotlib axes object \"\"\" assert plot_per . lower () . strip () in { \"variable\" , \"index\" , \"indices\" } indices_names = indices . values . columns variable_names = indices . values . index data = indices . runs . stack () . reset_index () data . rename ( columns = { \"level_0\" : \"variable\" , \"level_1\" : \"index\" , 0 : \"value\" }, inplace = True ) if plot_per == \"variable\" : col = \"variable\" x = \"index\" order = indices_names else : col = \"index\" x = \"variable\" order = variable_names ax = sns . catplot ( data = data , x = x , y = \"value\" , col = col , kind = kind , col_wrap = col_wrap , order = order , ** kwargs ) ax . set ( ylim = ( 0.0 , 1.0 )) ax . set_xticklabels ( rotation = 45 , horizontalalignment = \"right\" ) return ax","title":"cat_plot()"},{"location":"api/visualization/#deel.fairsense.visualization.text.format_with_intervals","text":"Pretty print the indices table with confidence intervals. Note that the intervals are displayed even if the indices are computed without confidence intervals. See :mod: fairsense.indices.confidence_intervals for more information. Parameters: Name Type Description Default indices_outputs IndicesOutput computed indices quantile (float): quantile used to compute confidence intervals. Values must be in [0., 0.5]. required the table with indices properly displayed. Note that the table values Type Description are now string and not float. Source code in deel\\fairsense\\visualization\\text.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def format_with_intervals ( indices_outputs : IndicesOutput , quantile : float = 0.05 ): \"\"\" Pretty print the indices table with confidence intervals. Note that the intervals are displayed even if the indices are computed without confidence intervals. See :mod:`fairsense.indices.confidence_intervals` for more information. Args: indices_outputs (IndicesOutput): computed indices quantile (float): quantile used to compute confidence intervals. Values must be in [0., 0.5]. Returns: the table with indices properly displayed. Note that the table values are now string and not float. \"\"\" means = indices_outputs . runs . groupby ( level = 0 ) . median () . clip ( 0.0 , 1.0 ) low = ( indices_outputs . runs . groupby ( level = 0 ) . aggregate ( partial ( np . quantile , q = quantile )) . clip ( 0.0 , 1.0 ) ) high = ( indices_outputs . runs . groupby ( level = 0 ) . aggregate ( partial ( np . quantile , q = 1 - quantile )) . clip ( 0.0 , 1.0 ) ) table = means . copy () for index in means . columns : table [ index ] = np . vectorize ( lambda index_val , index_inf_val , index_sup_val : \" %.2f [ %.2f , %.2f ]\" % ( index_val , index_inf_val , index_sup_val ) )( means [ index ], low [ index ], high [ index ]) return table","title":"format_with_intervals()"},{"location":"notebooks/demo-classification/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Demo: Fairness on classification problems \u00b6 # !pip install -e fairsense import matplotlib.pyplot as plt import numpy as np import pandas as pd from deel.fairsense.data_management.factory import from_numpy , from_pandas from deel.fairsense.data_management.processing import one_hot_encode from deel.fairsense.indices.confidence_intervals import with_confidence_intervals from deel.fairsense.indices.cvm import cvm_indices from deel.fairsense.indices.standard_metrics import disparate_impact from deel.fairsense.indices.sobol import sobol_indices from deel.fairsense.utils.dataclasses import IndicesInput from deel.fairsense.utils.fairness_objective import y_true , squared_error , y_pred , classification_error from deel.fairsense.visualization.plots import cat_plot from deel.fairsense.visualization.text import format_with_intervals from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score 1 study of the intrinsic fairness of the dataset \u00b6 data wrangling \u00b6 in this notebook we will work on the adult dataset. data = pd . read_csv ( \"data/adult.csv\" ) data [ \"income\" ] = data [ \"income\" ] == \">50K\" data = data . drop ([ 'native-country' , 'fnlwgt' ], axis = 1 ) # data = data[data[\"native-country\"] != \"Holand-Netherlands\"] data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass education educational-num marital-status occupation relationship race gender capital-gain capital-loss hours-per-week income 0 25 Private 11th 7 Never-married Machine-op-inspct Own-child Black Male 0 0 40 False 1 38 Private HS-grad 9 Married-civ-spouse Farming-fishing Husband White Male 0 0 50 False 2 28 Local-gov Assoc-acdm 12 Married-civ-spouse Protective-serv Husband White Male 0 0 40 True 3 44 Private Some-college 10 Married-civ-spouse Machine-op-inspct Husband Black Male 7688 0 40 True 4 18 ? Some-college 10 Never-married ? Own-child White Female 0 0 30 False indice computation: Disparate impact \u00b6 First we will start with computing some indices on the training data to see if the dataset is biased. The first step consist of building the IndicesInput object that stores the data. As we can set the target y_true means that we analyse the data, but this can be set to y_pred if we want to analyse predictions, or squared_error if we want to analyse the error. We can then apply preprocessing such as one_hot encoding. indices_inputs = from_pandas ( data , \"income\" , target = y_true ) categorical_cols = list ( filter ( lambda col : data . dtypes [ col ] == \"O\" , data . columns )) indices_inputs = one_hot_encode ( indices_inputs , categorical_cols ) We then declare the indices computation functions. The results are stored in a indicesOuput object. raw value can be acessed with .values , note that 0 refers to total independence and 1 refers to total dependence. indices_outputs = disparate_impact ( indices_inputs ) indices_outputs . values .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DI age NaN capital-gain NaN capital-loss NaN education 0.591701 educational-num NaN gender 0.640345 hours-per-week NaN marital-status 0.682576 occupation 0.439293 race 0.380506 relationship 0.747347 workclass 0.412769 cat_plot ( indices_outputs , plot_per = \"index\" , kind = \"bar\" ) plt . show () It is also possible to decorate any indice function with with_confidence_intervals to use bootstrapping to compute confidence intervals. We can also use the + operator to compute multiple indices simulteanously. Results with confidence intervals can be visualized either textually with format_with_intervals or 'graphically with cat_plot di_with_ci = with_confidence_intervals ( n_splits = 30 )( disparate_impact ) indices_outputs = di_with_ci ( indices_inputs ) format_with_intervals ( indices_outputs , quantile = 0.05 ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 30/30 [00:02<00:00, 11.66it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DI age nan [nan, nan] capital-gain nan [nan, nan] capital-loss nan [nan, nan] education 0.60 [0.54, 0.65] educational-num nan [nan, nan] gender 0.64 [0.58, 0.71] hours-per-week nan [nan, nan] marital-status 0.73 [0.54, 0.84] occupation 0.46 [0.40, 0.53] race 0.42 [0.26, 0.56] relationship 0.76 [0.69, 0.79] workclass 0.35 [0.19, 0.47] cat_plot ( indices_outputs , plot_per = \"index\" , kind = \"bar\" ) plt . show () 2 train a model and analyse it's sensitivity \u00b6 first we will split the data and then train a basic model on it. data = data . sample ( frac = 1. ) # shuffle data data_train = data . iloc [: int ( len ( data ) * 0.8 )] data_test = data . iloc [ int ( len ( data ) * 0.8 ):] similarly we build the IndiceInput object indices_inputs_train = IndicesInput ( x = indices_inputs . x . iloc [: int ( len ( data ) * 0.8 )], y_true = indices_inputs . y_true . iloc [: int ( len ( data ) * 0.8 )], variable_groups = indices_inputs . variable_groups ) indices_inputs_test = IndicesInput ( x = indices_inputs . x . iloc [ int ( len ( data ) * 0.8 ):], y_true = indices_inputs . y_true . iloc [ int ( len ( data ) * 0.8 ):], variable_groups = indices_inputs . variable_groups ) then we train a basic model: DecisionTree. Note that this analysis can be applied to any callable that can handle numpy array as inputs. model = DecisionTreeClassifier () model . fit ( indices_inputs_train . x , indices_inputs_train . y_true ) train_acc = accuracy_score ( indices_inputs_train . y_true , model . predict ( indices_inputs_train . x )) val_acc = accuracy_score ( indices_inputs_test . y_true , model . predict ( indices_inputs_test . x )) print ( f \"train acc: { train_acc } , val acc { val_acc } \" ) train acc: 0.9718987536150283, val acc 0.8328385709898659 we set the model and the objective indices_inputs_train . model = model . predict indices_inputs_train . objective = y_pred indices_inputs_test . model = model . predict indices_inputs_test . objective = y_pred indices_inputs . model = model . predict indices_inputs . objective = y_pred cvm_with_ci = with_confidence_intervals ( n_splits = 30 )( cvm_indices ) di_with_ci = with_confidence_intervals ( n_splits = 30 )( disparate_impact ) sobol_with_ci = with_confidence_intervals ( n_splits = 30 )( sobol_indices ) indices_outputs_test = cvm_with_ci ( indices_inputs_train ) + di_with_ci ( indices_inputs_train ) # + sobol_with_ci(indices_inputs_train) format_with_intervals ( indices_outputs_test , quantile = 0.1 ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 30/30 [01:10<00:00, 2.36s/it] 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 30/30 [00:01<00:00, 16.35it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVM CVM_indep DI age 0.97 [0.94, 0.98] 0.24 [0.20, 0.29] nan [nan, nan] capital-gain 0.99 [0.95, 1.00] 0.01 [0.00, 0.02] nan [nan, nan] capital-loss 0.99 [0.96, 1.00] 0.00 [0.00, 0.02] nan [nan, nan] education 1.00 [0.97, 1.00] 0.01 [0.00, 0.04] 0.61 [0.56, 0.66] educational-num 0.99 [0.97, 1.00] 0.05 [0.03, 0.07] nan [nan, nan] gender 0.99 [0.96, 1.00] 0.01 [0.00, 0.02] 0.62 [0.55, 0.69] hours-per-week 0.97 [0.96, 1.00] 0.09 [0.05, 0.12] nan [nan, nan] marital-status 1.00 [0.96, 1.00] 0.01 [0.00, 0.03] 0.68 [0.57, 0.80] occupation 0.97 [0.94, 0.99] 0.02 [0.00, 0.04] 0.47 [0.44, 0.55] race 0.99 [0.97, 1.00] 0.00 [0.00, 0.02] 0.40 [0.30, 0.57] relationship 0.99 [0.97, 1.00] 0.01 [0.00, 0.04] 0.75 [0.70, 0.78] workclass 0.99 [0.96, 1.00] 0.01 [0.00, 0.03] 0.30 [0.22, 0.42] cat_plot ( indices_outputs_test , plot_per = \"index\" , kind = \"box\" , col_wrap = 3 ) plt . show () indices_inputs_train . _objective = classification_error indices_inputs_test . _objective = classification_error cvm_with_ci = with_confidence_intervals ( n_splits = 15 )( cvm_indices ) sobol_with_ci = with_confidence_intervals ( n_splits = 15 )( sobol_indices ) indices_outputs_test_error = cvm_with_ci ( indices_inputs_test ) # + sobol_with_ci(indices_inputs_test) format_with_intervals ( indices_outputs_test_error , quantile = 0.1 ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 15/15 [00:11<00:00, 1.30it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVM CVM_indep age 0.98 [0.93, 1.00] 0.21 [0.18, 0.27] capital-gain 0.99 [0.95, 1.00] 0.00 [0.00, 0.01] capital-loss 1.00 [0.96, 1.00] 0.00 [0.00, 0.02] education 1.00 [0.96, 1.00] 0.02 [0.00, 0.04] educational-num 1.00 [0.97, 1.00] 0.05 [0.03, 0.07] gender 1.00 [0.96, 1.00] 0.00 [0.00, 0.02] hours-per-week 0.97 [0.95, 1.00] 0.09 [0.02, 0.13] marital-status 1.00 [0.97, 1.00] 0.01 [0.00, 0.03] occupation 0.99 [0.96, 1.00] 0.01 [0.00, 0.04] race 1.00 [0.97, 1.00] 0.00 [0.00, 0.02] relationship 0.99 [0.97, 1.00] 0.01 [0.00, 0.03] workclass 1.00 [0.96, 1.00] 0.01 [0.00, 0.03] cat_plot ( indices_outputs_test_error , plot_per = \"variable\" , kind = \"box\" , col_wrap = 4 ) plt . show ()","title":"Demo 2: Classification problem"},{"location":"notebooks/demo-classification/#demo-fairness-on-classification-problems","text":"# !pip install -e fairsense import matplotlib.pyplot as plt import numpy as np import pandas as pd from deel.fairsense.data_management.factory import from_numpy , from_pandas from deel.fairsense.data_management.processing import one_hot_encode from deel.fairsense.indices.confidence_intervals import with_confidence_intervals from deel.fairsense.indices.cvm import cvm_indices from deel.fairsense.indices.standard_metrics import disparate_impact from deel.fairsense.indices.sobol import sobol_indices from deel.fairsense.utils.dataclasses import IndicesInput from deel.fairsense.utils.fairness_objective import y_true , squared_error , y_pred , classification_error from deel.fairsense.visualization.plots import cat_plot from deel.fairsense.visualization.text import format_with_intervals from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score","title":"Demo: Fairness on classification problems"},{"location":"notebooks/demo-classification/#1-study-of-the-intrinsic-fairness-of-the-dataset","text":"","title":"1 study of the intrinsic fairness of the dataset"},{"location":"notebooks/demo-classification/#data-wrangling","text":"in this notebook we will work on the adult dataset. data = pd . read_csv ( \"data/adult.csv\" ) data [ \"income\" ] = data [ \"income\" ] == \">50K\" data = data . drop ([ 'native-country' , 'fnlwgt' ], axis = 1 ) # data = data[data[\"native-country\"] != \"Holand-Netherlands\"] data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass education educational-num marital-status occupation relationship race gender capital-gain capital-loss hours-per-week income 0 25 Private 11th 7 Never-married Machine-op-inspct Own-child Black Male 0 0 40 False 1 38 Private HS-grad 9 Married-civ-spouse Farming-fishing Husband White Male 0 0 50 False 2 28 Local-gov Assoc-acdm 12 Married-civ-spouse Protective-serv Husband White Male 0 0 40 True 3 44 Private Some-college 10 Married-civ-spouse Machine-op-inspct Husband Black Male 7688 0 40 True 4 18 ? Some-college 10 Never-married ? Own-child White Female 0 0 30 False","title":"data wrangling"},{"location":"notebooks/demo-classification/#indice-computation-disparate-impact","text":"First we will start with computing some indices on the training data to see if the dataset is biased. The first step consist of building the IndicesInput object that stores the data. As we can set the target y_true means that we analyse the data, but this can be set to y_pred if we want to analyse predictions, or squared_error if we want to analyse the error. We can then apply preprocessing such as one_hot encoding. indices_inputs = from_pandas ( data , \"income\" , target = y_true ) categorical_cols = list ( filter ( lambda col : data . dtypes [ col ] == \"O\" , data . columns )) indices_inputs = one_hot_encode ( indices_inputs , categorical_cols ) We then declare the indices computation functions. The results are stored in a indicesOuput object. raw value can be acessed with .values , note that 0 refers to total independence and 1 refers to total dependence. indices_outputs = disparate_impact ( indices_inputs ) indices_outputs . values .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DI age NaN capital-gain NaN capital-loss NaN education 0.591701 educational-num NaN gender 0.640345 hours-per-week NaN marital-status 0.682576 occupation 0.439293 race 0.380506 relationship 0.747347 workclass 0.412769 cat_plot ( indices_outputs , plot_per = \"index\" , kind = \"bar\" ) plt . show () It is also possible to decorate any indice function with with_confidence_intervals to use bootstrapping to compute confidence intervals. We can also use the + operator to compute multiple indices simulteanously. Results with confidence intervals can be visualized either textually with format_with_intervals or 'graphically with cat_plot di_with_ci = with_confidence_intervals ( n_splits = 30 )( disparate_impact ) indices_outputs = di_with_ci ( indices_inputs ) format_with_intervals ( indices_outputs , quantile = 0.05 ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 30/30 [00:02<00:00, 11.66it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DI age nan [nan, nan] capital-gain nan [nan, nan] capital-loss nan [nan, nan] education 0.60 [0.54, 0.65] educational-num nan [nan, nan] gender 0.64 [0.58, 0.71] hours-per-week nan [nan, nan] marital-status 0.73 [0.54, 0.84] occupation 0.46 [0.40, 0.53] race 0.42 [0.26, 0.56] relationship 0.76 [0.69, 0.79] workclass 0.35 [0.19, 0.47] cat_plot ( indices_outputs , plot_per = \"index\" , kind = \"bar\" ) plt . show ()","title":"indice computation: Disparate impact"},{"location":"notebooks/demo-classification/#2-train-a-model-and-analyse-its-sensitivity","text":"first we will split the data and then train a basic model on it. data = data . sample ( frac = 1. ) # shuffle data data_train = data . iloc [: int ( len ( data ) * 0.8 )] data_test = data . iloc [ int ( len ( data ) * 0.8 ):] similarly we build the IndiceInput object indices_inputs_train = IndicesInput ( x = indices_inputs . x . iloc [: int ( len ( data ) * 0.8 )], y_true = indices_inputs . y_true . iloc [: int ( len ( data ) * 0.8 )], variable_groups = indices_inputs . variable_groups ) indices_inputs_test = IndicesInput ( x = indices_inputs . x . iloc [ int ( len ( data ) * 0.8 ):], y_true = indices_inputs . y_true . iloc [ int ( len ( data ) * 0.8 ):], variable_groups = indices_inputs . variable_groups ) then we train a basic model: DecisionTree. Note that this analysis can be applied to any callable that can handle numpy array as inputs. model = DecisionTreeClassifier () model . fit ( indices_inputs_train . x , indices_inputs_train . y_true ) train_acc = accuracy_score ( indices_inputs_train . y_true , model . predict ( indices_inputs_train . x )) val_acc = accuracy_score ( indices_inputs_test . y_true , model . predict ( indices_inputs_test . x )) print ( f \"train acc: { train_acc } , val acc { val_acc } \" ) train acc: 0.9718987536150283, val acc 0.8328385709898659 we set the model and the objective indices_inputs_train . model = model . predict indices_inputs_train . objective = y_pred indices_inputs_test . model = model . predict indices_inputs_test . objective = y_pred indices_inputs . model = model . predict indices_inputs . objective = y_pred cvm_with_ci = with_confidence_intervals ( n_splits = 30 )( cvm_indices ) di_with_ci = with_confidence_intervals ( n_splits = 30 )( disparate_impact ) sobol_with_ci = with_confidence_intervals ( n_splits = 30 )( sobol_indices ) indices_outputs_test = cvm_with_ci ( indices_inputs_train ) + di_with_ci ( indices_inputs_train ) # + sobol_with_ci(indices_inputs_train) format_with_intervals ( indices_outputs_test , quantile = 0.1 ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 30/30 [01:10<00:00, 2.36s/it] 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 30/30 [00:01<00:00, 16.35it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVM CVM_indep DI age 0.97 [0.94, 0.98] 0.24 [0.20, 0.29] nan [nan, nan] capital-gain 0.99 [0.95, 1.00] 0.01 [0.00, 0.02] nan [nan, nan] capital-loss 0.99 [0.96, 1.00] 0.00 [0.00, 0.02] nan [nan, nan] education 1.00 [0.97, 1.00] 0.01 [0.00, 0.04] 0.61 [0.56, 0.66] educational-num 0.99 [0.97, 1.00] 0.05 [0.03, 0.07] nan [nan, nan] gender 0.99 [0.96, 1.00] 0.01 [0.00, 0.02] 0.62 [0.55, 0.69] hours-per-week 0.97 [0.96, 1.00] 0.09 [0.05, 0.12] nan [nan, nan] marital-status 1.00 [0.96, 1.00] 0.01 [0.00, 0.03] 0.68 [0.57, 0.80] occupation 0.97 [0.94, 0.99] 0.02 [0.00, 0.04] 0.47 [0.44, 0.55] race 0.99 [0.97, 1.00] 0.00 [0.00, 0.02] 0.40 [0.30, 0.57] relationship 0.99 [0.97, 1.00] 0.01 [0.00, 0.04] 0.75 [0.70, 0.78] workclass 0.99 [0.96, 1.00] 0.01 [0.00, 0.03] 0.30 [0.22, 0.42] cat_plot ( indices_outputs_test , plot_per = \"index\" , kind = \"box\" , col_wrap = 3 ) plt . show () indices_inputs_train . _objective = classification_error indices_inputs_test . _objective = classification_error cvm_with_ci = with_confidence_intervals ( n_splits = 15 )( cvm_indices ) sobol_with_ci = with_confidence_intervals ( n_splits = 15 )( sobol_indices ) indices_outputs_test_error = cvm_with_ci ( indices_inputs_test ) # + sobol_with_ci(indices_inputs_test) format_with_intervals ( indices_outputs_test_error , quantile = 0.1 ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 15/15 [00:11<00:00, 1.30it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVM CVM_indep age 0.98 [0.93, 1.00] 0.21 [0.18, 0.27] capital-gain 0.99 [0.95, 1.00] 0.00 [0.00, 0.01] capital-loss 1.00 [0.96, 1.00] 0.00 [0.00, 0.02] education 1.00 [0.96, 1.00] 0.02 [0.00, 0.04] educational-num 1.00 [0.97, 1.00] 0.05 [0.03, 0.07] gender 1.00 [0.96, 1.00] 0.00 [0.00, 0.02] hours-per-week 0.97 [0.95, 1.00] 0.09 [0.02, 0.13] marital-status 1.00 [0.97, 1.00] 0.01 [0.00, 0.03] occupation 0.99 [0.96, 1.00] 0.01 [0.00, 0.04] race 1.00 [0.97, 1.00] 0.00 [0.00, 0.02] relationship 0.99 [0.97, 1.00] 0.01 [0.00, 0.03] workclass 1.00 [0.96, 1.00] 0.01 [0.00, 0.03] cat_plot ( indices_outputs_test_error , plot_per = \"variable\" , kind = \"box\" , col_wrap = 4 ) plt . show ()","title":"2 train a model and analyse it's sensitivity"},{"location":"notebooks/demo-regression/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Demo: Fairness on regression problems \u00b6 # !pip install -e fairsense import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd from deel.fairsense.data_management.factory import from_numpy , from_pandas from deel.fairsense.data_management.processing import one_hot_encode from deel.fairsense.indices.confidence_intervals import with_confidence_intervals from deel.fairsense.indices.cvm import cvm_indices from deel.fairsense.indices.standard_metrics import disparate_impact from deel.fairsense.indices.sobol import sobol_indices from deel.fairsense.utils.dataclasses import IndicesInput , IndicesOutput from deel.fairsense.utils.fairness_objective import y_true , squared_error , y_pred from deel.fairsense.visualization.plots import cat_plot from deel.fairsense.visualization.text import format_with_intervals from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import accuracy_score from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split I) Study of the intrinsic fairness of the dataset \u00b6 data wrangling \u00b6 in this notebook we will work on the boston housing dataset. First we will start with computing some indices on the training data to see if the dataset is biased. The first step consist of building the IndicesInput object that stores the data. As we can set the target y_true means that we analyse the data, but this can be set to y_pred if we want to analyse predictions, or squared_error if we want to analyse the error. This parameter can be changer afterward. data = load_boston () # construct IndicesInput object indices_inputs = from_numpy ( x = data . data , y = data . target , feature_names = data . feature_names , target = y_true , ) indices_inputs . x . head () C:\\Users\\thibaut.boissin\\AppData\\Local\\Continuum\\anaconda3\\envs\\global_sensitivity_analysis_fairness\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2. The Boston housing prices dataset has an ethical problem. You can refer to the documentation of this function for further details. The scikit-learn maintainers therefore strongly discourage the use of this dataset unless the purpose of the code is to study and educate about ethical issues in data science and machine learning. In this special case, you can fetch the dataset from the original source:: import pandas as pd import numpy as np data_url = \"http://lib.stat.cmu.edu/datasets/boston\" raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None) data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]]) target = raw_df.values[1::2, 2] Alternative datasets include the California housing dataset (i.e. :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing dataset. You can load the datasets as follows:: from sklearn.datasets import fetch_california_housing housing = fetch_california_housing() for the California housing dataset and:: from sklearn.datasets import fetch_openml housing = fetch_openml(name=\"house_prices\", as_frame=True) for the Ames housing dataset. warnings.warn(msg, category=FutureWarning) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 We can then apply preprocessing such as one_hot encoding. # apply one hot encoding indices_inputs = one_hot_encode ( indices_inputs , [ \"CHAS\" , \"RAD\" ]) indices computation: CVM \u00b6 As we have a regression problem, we use the CVM indices to compute sensitvity analysis. We then declare the indices computation functions. The results are stored in a indicesOuput object. raw value can be acessed with .values , Please note that 0 refers to total independence and 1 refers to total dependence. indices_outputs = cvm_indices ( indices_inputs ) indices_outputs . values .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVM CVM_indep AGE 0.376566 0.000000 B 0.367895 0.000000 CHAS 0.412057 0.000000 CRIM 0.411401 0.000000 DIS 0.411260 0.000000 INDUS 0.410217 0.001823 LSTAT 0.475224 0.062403 NOX 0.412057 0.000000 PTRATIO 0.412104 0.001002 RAD 0.412057 0.000000 RM 0.414307 0.001800 TAX 0.432609 0.017128 ZN 0.402765 0.000000 We can now plot those easily using the approriate function from the visualization module. The two main parameters are plot_per and kind : plot_per (str): can be either variable or index , when set to variable there is one graph per variable, each graph showing the values of all indices. Respectively setting to index will build one graph per index, each showing the values for all variable. kind (str): kind of visualization to produce, can be one of strip , swarm , box , violin , boxen , point , bar . feel free to play with it ! cat_plot ( indices_outputs , plot_per = \"index\" , kind = \"bar\" ) plt . show () confidence intervals \u00b6 It is also possible to decorate any indice function with with_confidence_intervals to use bootstrapping to compute confidence intervals. We can also use the + operator to compute multiple indices simulteanously. Results with confidence intervals can be visualized either textually with format_with_intervals or 'graphically with cat_plot cvm_with_ci = with_confidence_intervals ( n_splits = 10 )( cvm_indices ) indices_outputs_ci = cvm_with_ci ( indices_inputs ) format_with_intervals ( indices_outputs_ci , quantile = 0.05 ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 10/10 [00:01<00:00, 7.95it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVM CVM_indep AGE 0.66 [0.49, 0.77] 0.00 [0.00, 0.08] B 0.51 [0.47, 0.85] 0.00 [0.00, 0.09] CHAS 0.64 [0.48, 0.85] 0.00 [0.00, 0.00] CRIM 0.60 [0.48, 0.86] 0.00 [0.00, 0.08] DIS 0.64 [0.48, 0.85] 0.00 [0.00, 0.00] INDUS 0.64 [0.47, 0.87] 0.00 [0.00, 0.04] LSTAT 0.73 [0.51, 0.87] 0.03 [0.00, 0.19] NOX 0.64 [0.48, 0.85] 0.00 [0.00, 0.00] PTRATIO 0.64 [0.48, 0.85] 0.00 [0.00, 0.00] RAD 0.64 [0.48, 0.85] 0.00 [0.00, 0.00] RM 0.64 [0.48, 0.85] 0.00 [0.00, 0.00] TAX 0.68 [0.45, 0.87] 0.01 [0.00, 0.17] ZN 0.64 [0.49, 0.84] 0.00 [0.00, 0.02] cat_plot ( indices_outputs_ci , plot_per = \"index\" , kind = \"box\" ) plt . show () II) train a model and analyse it's sensitivity \u00b6 train the model \u00b6 first we will split the data and then train a basic model on it. X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , test_size = 0.2 , random_state = 42 ) similarly we build the IndiceInput object indices_inputs_train = from_numpy ( x = X_train , y = y_train , feature_names = data . feature_names , ) indices_inputs_test = from_numpy ( x = X_test , y = y_test , feature_names = data . feature_names , ) then we train a basic model: DecisionTree. Note that this analysis can be applied to any callable that can handle numpy array as inputs. model = RandomForestRegressor ( 250 , max_depth = 5 , min_samples_leaf = 3 ) model . fit ( indices_inputs_train . x , indices_inputs_train . y_true ) train_score = model . score ( indices_inputs_train . x , indices_inputs_train . y_true ) val_score = model . score ( indices_inputs_test . x , indices_inputs_test . y_true ) print ( f \"train score: { train_score } , val score { val_score } \" ) C:\\Users\\thibaut.boissin\\AppData\\Local\\Continuum\\anaconda3\\envs\\global_sensitivity_analysis_fairness\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). train score: 0.9169602480149448, val score 0.8509534131406158 compute indices \u00b6 we set the model and the objective indices_inputs_train . model = model . predict indices_inputs_train . objective = y_pred indices_inputs_test . model = model . predict indices_inputs_test . objective = y_pred cvm_with_ci = with_confidence_intervals ( n_splits = 10 )( cvm_indices ) sobol_with_ci = with_confidence_intervals ( n_splits = 10 )( sobol_indices ) indices_outputs_train = cvm_with_ci ( indices_inputs_train ) + sobol_with_ci ( indices_inputs_train ) format_with_intervals ( indices_outputs_train , quantile = 0.1 ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 10/10 [00:01<00:00, 7.46it/s] 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 10/10 [00:26<00:00, 2.60s/it] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVM CVM_indep S ST S_ind ST_ind AGE 0.96 [0.88, 1.00] 0.01 [0.00, 0.10] 0.16 [0.07, 0.22] 0.26 [0.16, 0.32] 0.01 [0.00, 0.03] 0.01 [0.01, 0.02] B 1.00 [0.83, 1.00] 0.07 [0.00, 0.21] 0.03 [0.00, 0.11] 0.15 [0.04, 0.22] 0.00 [0.00, 0.02] 0.01 [0.00, 0.02] CHAS 0.97 [0.83, 1.00] 0.00 [0.00, 0.00] 0.03 [0.00, 0.09] 0.07 [0.04, 0.22] 0.00 [0.00, 0.02] 0.01 [0.00, 0.02] CRIM 0.99 [0.81, 1.00] 0.00 [0.00, 0.04] 0.16 [0.05, 0.28] 0.31 [0.14, 0.44] 0.00 [0.00, 0.01] 0.01 [0.01, 0.02] DIS 0.98 [0.83, 1.00] 0.00 [0.00, 0.00] 0.09 [0.01, 0.19] 0.23 [0.12, 0.34] 0.00 [0.00, 0.01] 0.01 [0.00, 0.02] INDUS 0.99 [0.80, 1.00] 0.00 [0.00, 0.02] 0.26 [0.17, 0.31] 0.40 [0.35, 0.51] 0.00 [0.00, 0.03] 0.01 [0.00, 0.01] LSTAT 0.98 [0.84, 1.00] 0.00 [0.00, 0.00] 0.68 [0.54, 0.84] 0.82 [0.74, 0.88] 0.02 [0.00, 0.11] 0.10 [0.08, 0.13] NOX 0.97 [0.83, 1.00] 0.00 [0.00, 0.00] 0.20 [0.03, 0.26] 0.33 [0.24, 0.44] 0.00 [0.00, 0.01] 0.01 [0.00, 0.02] PTRATIO 0.99 [0.83, 1.00] 0.00 [0.00, 0.00] 0.22 [0.14, 0.31] 0.34 [0.27, 0.59] 0.00 [0.00, 0.03] 0.03 [0.02, 0.04] RAD 0.97 [0.83, 1.00] 0.00 [0.00, 0.00] 0.10 [0.01, 0.31] 0.27 [0.18, 0.40] 0.00 [0.00, 0.01] 0.01 [0.01, 0.02] RM 0.98 [0.83, 1.00] 0.00 [0.00, 0.00] 0.67 [0.57, 0.81] 0.85 [0.82, 0.91] 0.10 [0.04, 0.13] 0.23 [0.20, 0.26] TAX 0.98 [0.79, 1.00] 0.01 [0.00, 0.06] 0.17 [0.05, 0.30] 0.32 [0.22, 0.46] 0.00 [0.00, 0.02] 0.01 [0.00, 0.02] ZN 1.00 [0.88, 1.00] 0.01 [0.00, 0.11] 0.12 [0.05, 0.27] 0.24 [0.18, 0.34] 0.01 [0.00, 0.01] 0.01 [0.00, 0.01] cat_plot ( indices_outputs_train , plot_per = \"variable\" , kind = \"box\" , col_wrap = 4 ) plt . show () compare indices from target=y_true with indices from target=y_pred \u00b6 OK, these results are interesting but we would like to compare the indices obtained with target=y_true . merged_indices = indices_outputs_ci . runs merged_indices [[ \"CVM_model\" , \"CVM_indep_model\" ]] = indices_outputs_train . runs [[ \"CVM\" , \"CVM_indep\" ]] merged_indices = IndicesOutput ( merged_indices [[ \"CVM_model\" , \"CVM\" , \"CVM_indep_model\" , \"CVM_indep\" ]]) cat_plot ( merged_indices , plot_per = \"variable\" , kind = \"box\" , col_wrap = 4 ) plt . show () As we can see the model tend to increase the influence of many variables III) Analysis of the sensitivity of the error \u00b6 Now we want to see if some variable are influent with the error of model. indices_inputs_train . objective = squared_error indices_inputs_test . objective = squared_error cvm_with_ci = with_confidence_intervals ( n_splits = 30 )( cvm_indices ) indices_outputs_error_test = cvm_with_ci ( indices_inputs_test ) format_with_intervals ( indices_outputs_error_test , quantile = 0.1 ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 30/30 [00:03<00:00, 7.99it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVM CVM_indep AGE 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] B 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] CHAS 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] CRIM 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] DIS 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] INDUS 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] LSTAT 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] NOX 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] PTRATIO 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] RAD 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] RM 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] TAX 1.00 [1.00, 1.00] 0.00 [0.00, 0.38] ZN 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] cat_plot ( indices_outputs_error_test , plot_per = \"variable\" , kind = \"box\" , col_wrap = 4 ) plt . show ()","title":"Demo 1: Regression problem"},{"location":"notebooks/demo-regression/#demo-fairness-on-regression-problems","text":"# !pip install -e fairsense import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd from deel.fairsense.data_management.factory import from_numpy , from_pandas from deel.fairsense.data_management.processing import one_hot_encode from deel.fairsense.indices.confidence_intervals import with_confidence_intervals from deel.fairsense.indices.cvm import cvm_indices from deel.fairsense.indices.standard_metrics import disparate_impact from deel.fairsense.indices.sobol import sobol_indices from deel.fairsense.utils.dataclasses import IndicesInput , IndicesOutput from deel.fairsense.utils.fairness_objective import y_true , squared_error , y_pred from deel.fairsense.visualization.plots import cat_plot from deel.fairsense.visualization.text import format_with_intervals from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import accuracy_score from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split","title":"Demo: Fairness on regression problems"},{"location":"notebooks/demo-regression/#i-study-of-the-intrinsic-fairness-of-the-dataset","text":"","title":"I) Study of the intrinsic fairness of the dataset"},{"location":"notebooks/demo-regression/#data-wrangling","text":"in this notebook we will work on the boston housing dataset. First we will start with computing some indices on the training data to see if the dataset is biased. The first step consist of building the IndicesInput object that stores the data. As we can set the target y_true means that we analyse the data, but this can be set to y_pred if we want to analyse predictions, or squared_error if we want to analyse the error. This parameter can be changer afterward. data = load_boston () # construct IndicesInput object indices_inputs = from_numpy ( x = data . data , y = data . target , feature_names = data . feature_names , target = y_true , ) indices_inputs . x . head () C:\\Users\\thibaut.boissin\\AppData\\Local\\Continuum\\anaconda3\\envs\\global_sensitivity_analysis_fairness\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2. The Boston housing prices dataset has an ethical problem. You can refer to the documentation of this function for further details. The scikit-learn maintainers therefore strongly discourage the use of this dataset unless the purpose of the code is to study and educate about ethical issues in data science and machine learning. In this special case, you can fetch the dataset from the original source:: import pandas as pd import numpy as np data_url = \"http://lib.stat.cmu.edu/datasets/boston\" raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None) data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]]) target = raw_df.values[1::2, 2] Alternative datasets include the California housing dataset (i.e. :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing dataset. You can load the datasets as follows:: from sklearn.datasets import fetch_california_housing housing = fetch_california_housing() for the California housing dataset and:: from sklearn.datasets import fetch_openml housing = fetch_openml(name=\"house_prices\", as_frame=True) for the Ames housing dataset. warnings.warn(msg, category=FutureWarning) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 We can then apply preprocessing such as one_hot encoding. # apply one hot encoding indices_inputs = one_hot_encode ( indices_inputs , [ \"CHAS\" , \"RAD\" ])","title":"data wrangling"},{"location":"notebooks/demo-regression/#indices-computation-cvm","text":"As we have a regression problem, we use the CVM indices to compute sensitvity analysis. We then declare the indices computation functions. The results are stored in a indicesOuput object. raw value can be acessed with .values , Please note that 0 refers to total independence and 1 refers to total dependence. indices_outputs = cvm_indices ( indices_inputs ) indices_outputs . values .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVM CVM_indep AGE 0.376566 0.000000 B 0.367895 0.000000 CHAS 0.412057 0.000000 CRIM 0.411401 0.000000 DIS 0.411260 0.000000 INDUS 0.410217 0.001823 LSTAT 0.475224 0.062403 NOX 0.412057 0.000000 PTRATIO 0.412104 0.001002 RAD 0.412057 0.000000 RM 0.414307 0.001800 TAX 0.432609 0.017128 ZN 0.402765 0.000000 We can now plot those easily using the approriate function from the visualization module. The two main parameters are plot_per and kind : plot_per (str): can be either variable or index , when set to variable there is one graph per variable, each graph showing the values of all indices. Respectively setting to index will build one graph per index, each showing the values for all variable. kind (str): kind of visualization to produce, can be one of strip , swarm , box , violin , boxen , point , bar . feel free to play with it ! cat_plot ( indices_outputs , plot_per = \"index\" , kind = \"bar\" ) plt . show ()","title":"indices computation: CVM"},{"location":"notebooks/demo-regression/#confidence-intervals","text":"It is also possible to decorate any indice function with with_confidence_intervals to use bootstrapping to compute confidence intervals. We can also use the + operator to compute multiple indices simulteanously. Results with confidence intervals can be visualized either textually with format_with_intervals or 'graphically with cat_plot cvm_with_ci = with_confidence_intervals ( n_splits = 10 )( cvm_indices ) indices_outputs_ci = cvm_with_ci ( indices_inputs ) format_with_intervals ( indices_outputs_ci , quantile = 0.05 ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 10/10 [00:01<00:00, 7.95it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVM CVM_indep AGE 0.66 [0.49, 0.77] 0.00 [0.00, 0.08] B 0.51 [0.47, 0.85] 0.00 [0.00, 0.09] CHAS 0.64 [0.48, 0.85] 0.00 [0.00, 0.00] CRIM 0.60 [0.48, 0.86] 0.00 [0.00, 0.08] DIS 0.64 [0.48, 0.85] 0.00 [0.00, 0.00] INDUS 0.64 [0.47, 0.87] 0.00 [0.00, 0.04] LSTAT 0.73 [0.51, 0.87] 0.03 [0.00, 0.19] NOX 0.64 [0.48, 0.85] 0.00 [0.00, 0.00] PTRATIO 0.64 [0.48, 0.85] 0.00 [0.00, 0.00] RAD 0.64 [0.48, 0.85] 0.00 [0.00, 0.00] RM 0.64 [0.48, 0.85] 0.00 [0.00, 0.00] TAX 0.68 [0.45, 0.87] 0.01 [0.00, 0.17] ZN 0.64 [0.49, 0.84] 0.00 [0.00, 0.02] cat_plot ( indices_outputs_ci , plot_per = \"index\" , kind = \"box\" ) plt . show ()","title":"confidence intervals"},{"location":"notebooks/demo-regression/#ii-train-a-model-and-analyse-its-sensitivity","text":"","title":"II) train a model and analyse it's sensitivity"},{"location":"notebooks/demo-regression/#train-the-model","text":"first we will split the data and then train a basic model on it. X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , test_size = 0.2 , random_state = 42 ) similarly we build the IndiceInput object indices_inputs_train = from_numpy ( x = X_train , y = y_train , feature_names = data . feature_names , ) indices_inputs_test = from_numpy ( x = X_test , y = y_test , feature_names = data . feature_names , ) then we train a basic model: DecisionTree. Note that this analysis can be applied to any callable that can handle numpy array as inputs. model = RandomForestRegressor ( 250 , max_depth = 5 , min_samples_leaf = 3 ) model . fit ( indices_inputs_train . x , indices_inputs_train . y_true ) train_score = model . score ( indices_inputs_train . x , indices_inputs_train . y_true ) val_score = model . score ( indices_inputs_test . x , indices_inputs_test . y_true ) print ( f \"train score: { train_score } , val score { val_score } \" ) C:\\Users\\thibaut.boissin\\AppData\\Local\\Continuum\\anaconda3\\envs\\global_sensitivity_analysis_fairness\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). train score: 0.9169602480149448, val score 0.8509534131406158","title":"train the model"},{"location":"notebooks/demo-regression/#compute-indices","text":"we set the model and the objective indices_inputs_train . model = model . predict indices_inputs_train . objective = y_pred indices_inputs_test . model = model . predict indices_inputs_test . objective = y_pred cvm_with_ci = with_confidence_intervals ( n_splits = 10 )( cvm_indices ) sobol_with_ci = with_confidence_intervals ( n_splits = 10 )( sobol_indices ) indices_outputs_train = cvm_with_ci ( indices_inputs_train ) + sobol_with_ci ( indices_inputs_train ) format_with_intervals ( indices_outputs_train , quantile = 0.1 ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 10/10 [00:01<00:00, 7.46it/s] 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 10/10 [00:26<00:00, 2.60s/it] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVM CVM_indep S ST S_ind ST_ind AGE 0.96 [0.88, 1.00] 0.01 [0.00, 0.10] 0.16 [0.07, 0.22] 0.26 [0.16, 0.32] 0.01 [0.00, 0.03] 0.01 [0.01, 0.02] B 1.00 [0.83, 1.00] 0.07 [0.00, 0.21] 0.03 [0.00, 0.11] 0.15 [0.04, 0.22] 0.00 [0.00, 0.02] 0.01 [0.00, 0.02] CHAS 0.97 [0.83, 1.00] 0.00 [0.00, 0.00] 0.03 [0.00, 0.09] 0.07 [0.04, 0.22] 0.00 [0.00, 0.02] 0.01 [0.00, 0.02] CRIM 0.99 [0.81, 1.00] 0.00 [0.00, 0.04] 0.16 [0.05, 0.28] 0.31 [0.14, 0.44] 0.00 [0.00, 0.01] 0.01 [0.01, 0.02] DIS 0.98 [0.83, 1.00] 0.00 [0.00, 0.00] 0.09 [0.01, 0.19] 0.23 [0.12, 0.34] 0.00 [0.00, 0.01] 0.01 [0.00, 0.02] INDUS 0.99 [0.80, 1.00] 0.00 [0.00, 0.02] 0.26 [0.17, 0.31] 0.40 [0.35, 0.51] 0.00 [0.00, 0.03] 0.01 [0.00, 0.01] LSTAT 0.98 [0.84, 1.00] 0.00 [0.00, 0.00] 0.68 [0.54, 0.84] 0.82 [0.74, 0.88] 0.02 [0.00, 0.11] 0.10 [0.08, 0.13] NOX 0.97 [0.83, 1.00] 0.00 [0.00, 0.00] 0.20 [0.03, 0.26] 0.33 [0.24, 0.44] 0.00 [0.00, 0.01] 0.01 [0.00, 0.02] PTRATIO 0.99 [0.83, 1.00] 0.00 [0.00, 0.00] 0.22 [0.14, 0.31] 0.34 [0.27, 0.59] 0.00 [0.00, 0.03] 0.03 [0.02, 0.04] RAD 0.97 [0.83, 1.00] 0.00 [0.00, 0.00] 0.10 [0.01, 0.31] 0.27 [0.18, 0.40] 0.00 [0.00, 0.01] 0.01 [0.01, 0.02] RM 0.98 [0.83, 1.00] 0.00 [0.00, 0.00] 0.67 [0.57, 0.81] 0.85 [0.82, 0.91] 0.10 [0.04, 0.13] 0.23 [0.20, 0.26] TAX 0.98 [0.79, 1.00] 0.01 [0.00, 0.06] 0.17 [0.05, 0.30] 0.32 [0.22, 0.46] 0.00 [0.00, 0.02] 0.01 [0.00, 0.02] ZN 1.00 [0.88, 1.00] 0.01 [0.00, 0.11] 0.12 [0.05, 0.27] 0.24 [0.18, 0.34] 0.01 [0.00, 0.01] 0.01 [0.00, 0.01] cat_plot ( indices_outputs_train , plot_per = \"variable\" , kind = \"box\" , col_wrap = 4 ) plt . show ()","title":"compute indices"},{"location":"notebooks/demo-regression/#compare-indices-from-targety_true-with-indices-from-targety_pred","text":"OK, these results are interesting but we would like to compare the indices obtained with target=y_true . merged_indices = indices_outputs_ci . runs merged_indices [[ \"CVM_model\" , \"CVM_indep_model\" ]] = indices_outputs_train . runs [[ \"CVM\" , \"CVM_indep\" ]] merged_indices = IndicesOutput ( merged_indices [[ \"CVM_model\" , \"CVM\" , \"CVM_indep_model\" , \"CVM_indep\" ]]) cat_plot ( merged_indices , plot_per = \"variable\" , kind = \"box\" , col_wrap = 4 ) plt . show () As we can see the model tend to increase the influence of many variables","title":"compare indices from target=y_true with indices from target=y_pred"},{"location":"notebooks/demo-regression/#iii-analysis-of-the-sensitivity-of-the-error","text":"Now we want to see if some variable are influent with the error of model. indices_inputs_train . objective = squared_error indices_inputs_test . objective = squared_error cvm_with_ci = with_confidence_intervals ( n_splits = 30 )( cvm_indices ) indices_outputs_error_test = cvm_with_ci ( indices_inputs_test ) format_with_intervals ( indices_outputs_error_test , quantile = 0.1 ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 30/30 [00:03<00:00, 7.99it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CVM CVM_indep AGE 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] B 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] CHAS 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] CRIM 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] DIS 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] INDUS 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] LSTAT 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] NOX 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] PTRATIO 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] RAD 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] RM 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] TAX 1.00 [1.00, 1.00] 0.00 [0.00, 0.38] ZN 1.00 [1.00, 1.00] -0.00 [0.00, 0.00] cat_plot ( indices_outputs_error_test , plot_per = \"variable\" , kind = \"box\" , col_wrap = 4 ) plt . show ()","title":"III) Analysis of the sensitivity of the error"},{"location":"notebooks/quickstart/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Quick start: how to read indices ? \u00b6 # !pip install fairsense import numpy as np import pandas as pd import matplotlib.pyplot as plt from deel.fairsense.indices import cvm_indices , sobol_indices , with_confidence_intervals from deel.fairsense.utils.dataclasses import IndicesInput from deel.fairsense.utils.fairness_objective import y_pred from deel.fairsense.visualization import cat_plot , format_with_intervals The data \u00b6 In this example we will highlight sobol indices properties using a very simple distribution: a 3 variable gaussian distribution where the variances and covariances can be controlled. def gaussian_data_generator ( sigma12 , sigma13 , sigma23 , N , var1 = 1.0 , var2 = 1.0 , var3 = 1.0 ): cov = np . mat ( [[ var1 , sigma12 , sigma13 ], [ sigma12 , var2 , sigma23 ], [ sigma13 , sigma23 , var3 ]] ) x = np . random . multivariate_normal ( mean = np . array ([ 0 , 0 , 0 ]), cov = cov , size = N ) return pd . DataFrame ( x , columns = [ 0 , 1 , 2 ]) Intro: Computing the indices \u00b6 In order to compute the indices, we must start with building an IndiceInput object, which can be done by providing a dataset, a model and an objective. model = lambda x : x [ \"0\" ] # \"f(X_0, X_1, X_2) -> X_0\" data = gaussian_data_generator ( sigma12 = 0. , sigma13 = 0. , sigma23 = 0. , N = 10 ** 3 ) objective = y_pred inputs = IndicesInput ( model = model , x = data , objective = objective ) We can now compute indices using the functions provided in deel.fairsense.indices . Results can be stacked using the + operator. results = sobol_indices ( inputs , n = 10 ** 3 ) + cvm_indices ( inputs ) results . values . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } S ST S_ind ST_ind CVM CVM_indep 0 1.000000 1.000000 0.947373 0.961075 0.993082 0.891087 1 0.001122 0.000267 0.000237 0.000392 0.034323 0.000000 2 0.000000 0.002494 0.001184 0.000971 0.035442 0.000000 We can also enrich usual indices to compute confidence intervals (computed using k-fold over the data) sobol_with_ci = with_confidence_intervals ( n_splits = 31 )( sobol_indices ) results = sobol_with_ci ( inputs , n = 10 ** 3 ) format_with_intervals ( results ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 31/31 [00:02<00:00, 15.19it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } S ST S_ind ST_ind 0 0.98 [0.92, 1.00] 0.99 [0.94, 1.00] 0.93 [0.76, 1.00] 0.95 [0.81, 1.00] 1 0.03 [0.00, 0.12] 0.04 [0.01, 0.17] 0.00 [0.00, 0.00] 0.00 [0.00, 0.01] 2 0.02 [0.00, 0.10] 0.04 [0.00, 0.16] 0.00 [0.00, 0.00] 0.00 [0.00, 0.01] How to read the indices \u00b6 GSA indices quantify how much of a variable influences the variance of the output of a function. It is import to recall that a variable can be influent in numerous ways: a variable can be influential by itself a variable can be influential becaus it is correlated to an influent variable a variable can be influential because of a joint effect with another variable (ex: cold + no salt on road + rain => slippery road is an example where each variable is not influential by itself but is influential when joined with other ) There is 4 Sobol indices that allow to give clues about how a variable is influential. Sobol indices and input distribution \u00b6 for the function f(X_0, X_1, X_2) -> X_0 + X_1 , the sobol indices that X_0 and X_1 a both equally influential : model = lambda x : x [ \"0\" ] + x [ \"1\" ] + 0.001 * x [ \"2\" ] # \"f(X_0, X_1, X_2) -> X_0 + X_1\" data = gaussian_data_generator ( sigma12 = 0. , sigma13 = 0. , sigma23 = 0. , N = 10 ** 3 ) objective = y_pred inputs = IndicesInput ( model = model , x = data , objective = objective ) results = sobol_indices ( inputs , n = 10 ** 3 ) # + cvm_indices(inputs) cat_plot ( results , plot_per = \"index\" , kind = \"bar\" ) plt . show () However, it is important to recall that Sobol indices accounts for the input distribution. For instance when $ var(X_0) = 10 $ and $ var(X_1) = 1 $ the indices reveals that in practice $ X_0 $ is more influential than $ X_1 $: model = lambda x : x [ \"0\" ] + x [ \"1\" ] # \"f(X_0, X_1, X_2) -> X_0 + X_1\" data = gaussian_data_generator ( sigma12 = 0. , sigma13 = 0. , sigma23 = 0. , var1 = 10 , N = 10 ** 3 ) objective = y_pred inputs = IndicesInput ( model = model , x = data , objective = objective ) results = sobol_indices ( inputs , n = 10 ** 3 ) # + cvm_indices(inputs) cat_plot ( results , plot_per = \"index\" , kind = \"bar\" ) plt . show () Sobol indices and correlations \u00b6 When a variable is influential trough correlations, sobol indices account it's influence, while Sobol independent does not. This can be observed in this example where: $$ f(X_0, X_1, X_2) = X_0 $$ And \\(X_1\\) and \\(X_2\\) are both correlated to \\(X_0\\) with a 0.5 correlation coefficient. model = lambda x : x [ \"0\" ] # \"f(X_0, X_1, X_2) -> X_0\" data = gaussian_data_generator ( sigma12 = 0.5 , sigma13 = 0.5 , sigma23 = 0. , N = 10 ** 3 ) objective = y_pred inputs = IndicesInput ( model = model , x = data , objective = objective ) results = sobol_indices ( inputs , n = 10 ** 3 ) # + cvm_indices(inputs) cat_plot ( results , plot_per = \"index\" , kind = \"bar\" ) plt . show () Sobol indices and joint effects \u00b6 When a variable is influential trough joint effects, sobol total indices account it's influence, while Sobol does not. This can be observed in this example where: $$ f(X_0, X_1, X_2) = 20 X_0 \\text{ if } X_1 \\text{ and } X_2 \\leq 0.5 \\ = 1.0 X_0 \\text{ otherwise } $$ model = lambda x : x [ \"0\" ] * ((( x [ \"1\" ] > 0 ) * ( x [ \"2\" ] > 0 ) * 20 ) + - 10 ) # \"f(x) -> 20*X_0 if (X_1 > 0.5) && (X_2 > 0.5) else: 0.25*X_0 \" data = gaussian_data_generator ( sigma12 = 0. , sigma13 = 0. , sigma23 = 0. , N = 10 ** 3 ) objective = y_pred inputs = IndicesInput ( model = model , x = data , objective = objective ) results = sobol_indices ( inputs , n = 10 ** 3 ) # + cvm_indices(inputs) cat_plot ( results , plot_per = \"index\" , kind = \"bar\" ) plt . show ()","title":"Quick start: how to read indices"},{"location":"notebooks/quickstart/#quick-start-how-to-read-indices","text":"# !pip install fairsense import numpy as np import pandas as pd import matplotlib.pyplot as plt from deel.fairsense.indices import cvm_indices , sobol_indices , with_confidence_intervals from deel.fairsense.utils.dataclasses import IndicesInput from deel.fairsense.utils.fairness_objective import y_pred from deel.fairsense.visualization import cat_plot , format_with_intervals","title":"Quick start: how to read indices ?"},{"location":"notebooks/quickstart/#the-data","text":"In this example we will highlight sobol indices properties using a very simple distribution: a 3 variable gaussian distribution where the variances and covariances can be controlled. def gaussian_data_generator ( sigma12 , sigma13 , sigma23 , N , var1 = 1.0 , var2 = 1.0 , var3 = 1.0 ): cov = np . mat ( [[ var1 , sigma12 , sigma13 ], [ sigma12 , var2 , sigma23 ], [ sigma13 , sigma23 , var3 ]] ) x = np . random . multivariate_normal ( mean = np . array ([ 0 , 0 , 0 ]), cov = cov , size = N ) return pd . DataFrame ( x , columns = [ 0 , 1 , 2 ])","title":"The data"},{"location":"notebooks/quickstart/#intro-computing-the-indices","text":"In order to compute the indices, we must start with building an IndiceInput object, which can be done by providing a dataset, a model and an objective. model = lambda x : x [ \"0\" ] # \"f(X_0, X_1, X_2) -> X_0\" data = gaussian_data_generator ( sigma12 = 0. , sigma13 = 0. , sigma23 = 0. , N = 10 ** 3 ) objective = y_pred inputs = IndicesInput ( model = model , x = data , objective = objective ) We can now compute indices using the functions provided in deel.fairsense.indices . Results can be stacked using the + operator. results = sobol_indices ( inputs , n = 10 ** 3 ) + cvm_indices ( inputs ) results . values . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } S ST S_ind ST_ind CVM CVM_indep 0 1.000000 1.000000 0.947373 0.961075 0.993082 0.891087 1 0.001122 0.000267 0.000237 0.000392 0.034323 0.000000 2 0.000000 0.002494 0.001184 0.000971 0.035442 0.000000 We can also enrich usual indices to compute confidence intervals (computed using k-fold over the data) sobol_with_ci = with_confidence_intervals ( n_splits = 31 )( sobol_indices ) results = sobol_with_ci ( inputs , n = 10 ** 3 ) format_with_intervals ( results ) 100%|\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 31/31 [00:02<00:00, 15.19it/s] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } S ST S_ind ST_ind 0 0.98 [0.92, 1.00] 0.99 [0.94, 1.00] 0.93 [0.76, 1.00] 0.95 [0.81, 1.00] 1 0.03 [0.00, 0.12] 0.04 [0.01, 0.17] 0.00 [0.00, 0.00] 0.00 [0.00, 0.01] 2 0.02 [0.00, 0.10] 0.04 [0.00, 0.16] 0.00 [0.00, 0.00] 0.00 [0.00, 0.01]","title":"Intro: Computing the indices"},{"location":"notebooks/quickstart/#how-to-read-the-indices","text":"GSA indices quantify how much of a variable influences the variance of the output of a function. It is import to recall that a variable can be influent in numerous ways: a variable can be influential by itself a variable can be influential becaus it is correlated to an influent variable a variable can be influential because of a joint effect with another variable (ex: cold + no salt on road + rain => slippery road is an example where each variable is not influential by itself but is influential when joined with other ) There is 4 Sobol indices that allow to give clues about how a variable is influential.","title":"How to read the indices"},{"location":"notebooks/quickstart/#sobol-indices-and-input-distribution","text":"for the function f(X_0, X_1, X_2) -> X_0 + X_1 , the sobol indices that X_0 and X_1 a both equally influential : model = lambda x : x [ \"0\" ] + x [ \"1\" ] + 0.001 * x [ \"2\" ] # \"f(X_0, X_1, X_2) -> X_0 + X_1\" data = gaussian_data_generator ( sigma12 = 0. , sigma13 = 0. , sigma23 = 0. , N = 10 ** 3 ) objective = y_pred inputs = IndicesInput ( model = model , x = data , objective = objective ) results = sobol_indices ( inputs , n = 10 ** 3 ) # + cvm_indices(inputs) cat_plot ( results , plot_per = \"index\" , kind = \"bar\" ) plt . show () However, it is important to recall that Sobol indices accounts for the input distribution. For instance when $ var(X_0) = 10 $ and $ var(X_1) = 1 $ the indices reveals that in practice $ X_0 $ is more influential than $ X_1 $: model = lambda x : x [ \"0\" ] + x [ \"1\" ] # \"f(X_0, X_1, X_2) -> X_0 + X_1\" data = gaussian_data_generator ( sigma12 = 0. , sigma13 = 0. , sigma23 = 0. , var1 = 10 , N = 10 ** 3 ) objective = y_pred inputs = IndicesInput ( model = model , x = data , objective = objective ) results = sobol_indices ( inputs , n = 10 ** 3 ) # + cvm_indices(inputs) cat_plot ( results , plot_per = \"index\" , kind = \"bar\" ) plt . show ()","title":"Sobol indices and input distribution"},{"location":"notebooks/quickstart/#sobol-indices-and-correlations","text":"When a variable is influential trough correlations, sobol indices account it's influence, while Sobol independent does not. This can be observed in this example where: $$ f(X_0, X_1, X_2) = X_0 $$ And \\(X_1\\) and \\(X_2\\) are both correlated to \\(X_0\\) with a 0.5 correlation coefficient. model = lambda x : x [ \"0\" ] # \"f(X_0, X_1, X_2) -> X_0\" data = gaussian_data_generator ( sigma12 = 0.5 , sigma13 = 0.5 , sigma23 = 0. , N = 10 ** 3 ) objective = y_pred inputs = IndicesInput ( model = model , x = data , objective = objective ) results = sobol_indices ( inputs , n = 10 ** 3 ) # + cvm_indices(inputs) cat_plot ( results , plot_per = \"index\" , kind = \"bar\" ) plt . show ()","title":"Sobol indices and correlations"},{"location":"notebooks/quickstart/#sobol-indices-and-joint-effects","text":"When a variable is influential trough joint effects, sobol total indices account it's influence, while Sobol does not. This can be observed in this example where: $$ f(X_0, X_1, X_2) = 20 X_0 \\text{ if } X_1 \\text{ and } X_2 \\leq 0.5 \\ = 1.0 X_0 \\text{ otherwise } $$ model = lambda x : x [ \"0\" ] * ((( x [ \"1\" ] > 0 ) * ( x [ \"2\" ] > 0 ) * 20 ) + - 10 ) # \"f(x) -> 20*X_0 if (X_1 > 0.5) && (X_2 > 0.5) else: 0.25*X_0 \" data = gaussian_data_generator ( sigma12 = 0. , sigma13 = 0. , sigma23 = 0. , N = 10 ** 3 ) objective = y_pred inputs = IndicesInput ( model = model , x = data , objective = objective ) results = sobol_indices ( inputs , n = 10 ** 3 ) # + cvm_indices(inputs) cat_plot ( results , plot_per = \"index\" , kind = \"bar\" ) plt . show ()","title":"Sobol indices and joint effects"}]}